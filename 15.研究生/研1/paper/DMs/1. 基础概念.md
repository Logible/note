# 基础概念

- [基础概念](#基础概念)
  - [基本](#基本)
  - [density estimation and resolution](#density-estimation-and-resolution)
  - [A "reweighted objective"](#a-reweighted-objective)
  - [sample quality](#sample-quality)
  - [inductive model](#inductive-model)
  - [underlying neural backbone](#underlying-neural-backbone)
  - [3.1. Perceptual Image Compression](#31-perceptual-image-compression)
  - [3.2](#32)


## 基本

在训练方式上，DMs相较于GAN有着较大的差别，通过**对原始数据进行加噪使其接近高斯分布**，再利用神经网络去除图像中的噪声以实现图像生成，另外，神经网络能够**学习到原始图像的数据分布**，因此能够实现对原始图像数据做小的修改而实现更加多样化的图像生成。

- 扩散模型涉及两个互关联的过程，分别是前向过程与反向过程。

  - 前向过程将数据分布转换为更简单的先验分布，例如高斯分布；
  - 反向过程，利用经过训练的神经网络，通过**模拟普通或随机微分方程**实现前向过程逆过程[9]。

- Diffusion happens in multiple steps, each step operates on an input latents array, and **produces another latents array that better resembles the input text and all the visual information** the model picked up from all images the model was trained on.

## density estimation and resolution

- **density estimation**: Density estimation in the context of generative models is the **process of modeling the probability distribution of the data.** It involves understanding how likely different types of data are, which helps in generating new samples that are similar to the original dataset.

## A "reweighted objective" 

- refers to modifying the standard training objective or loss function by applying different weights **to certain parts of the data or certain aspects of the model's output.** 

## sample quality

- "sample quality"指的是生成的样本（例如图像或文本）的质量。

## inductive model

The term "inductive biases of image-like data" refers to **the inherent assumptions or predispositions that a machine learning model**, particularly one based on a UNet architecture, **has** when dealing with data that has characteristics similar to images. These biases influence how the model processes and learns from the data. 

**Spatial Hierarchies**: UNet, with its encoder-decoder structure, naturally captures the hierarchical spatial structure of images. It learns to **recognize patterns at different scales and locations**, which is crucial for understanding image-like data.

## underlying neural backbone

particularly when referring to **a UNet architecture,** describes the fundamental neural network structure that forms the core or the base of the model

## 3.1. Perceptual Image Compression

- **a perceptual loss function**
  - aims to preserve texture and other details that are perceptually significant to humans.
-  **a patch-based approach**
  -  assesses small parts or "patches" of the image. This method helps in focusing on local details and textures, enabling the autoencoder to generate images that are more realistic and have higher local fidelity.
- **Downsampling**: Downsampling is the process of reducing the resolution of an image. It involves decreasing the height and width of the image, thus reducing the total number of pixels.
- **High-variance latent spaces**
  -  a characteristic of the latent space in a generative model where **the representations (latent variables) have a large amount of variability or spread**. 
  - similar inputs may have **very different representations** in the latent space, 
- VAE(Variational Autoencoder)
  - VAEs are particularly known for their use of probabilistic encoders and decoders, which map **inputs to distributions over the latent space**, rather than deterministic points.
  - This KL penalty encourages the latent space **to approximate a chosen prior distribution**, often **a standard normal distribution** (a normal distribution with a mean of zero and a variance of one). 
  - By imposing this penalty, **VAEs aim to have a well-structured latent space** where similar inputs are encoded to similar points in the latent space, and where sampling from the latent space can generate realistic outputs.
- VQ
  - 向量量化（Vector Quantization, VQ）层是神经网络中的一种特殊层，它在编码器和解码器之间进行操作，将连续的潜在空间表示转换为离散的表示。具体来说，它涉及以下步骤：

    1. **量化代码本**: 在训练之初，定义一组离散的向量，这些向量组成了一个“代码本”或“字典”。这个代码本包含了多个向量，每个向量可以看作是潜在空间的一个“代码点”或“中心”。
    2. **最近邻搜索**: 编码器输出一个连续的潜在向量。向量量化层接收这个连续向量，并在代码本中寻找与之距离最近的代码点。  
    3. **映射**: 一旦找到最近的代码点，原始的连续向量就会被这个代码点替换，实现了向量的量化。这意味着，无论编码器的**输出**有多细微的变化，只要它们离同一个代码点最近，就会被量化成相同的向量
- a "two-dimensional structure"
  - refers to the width and height dimensions of an image or **a feature map**. If the latent space `z` retains this two-dimensional structure, it implies that the model **preserves spatial relationships and structures** that are present in the original image data, instead of flattening them into a one-dimensional vector.
    - A "feature map" refers to the output of one of the layers in a convolutional neural network (CNN). 

## 3.2

- variational lower bound
  - the goal of approximating the true data distribution p(x) as closely as possible by **optimizing the parameters of the model**.
- **似然（Likelihood）**
  - 指的是在**给定特定模型参数**下，**观察到当前数据的概率**。在统计学中，当我们有一组观察到的数据点时，我们会选择一个概率模型，并使用似然函数来估计模型参数。
- 最大化似然（Maximizing the Likelihood
  - 假设我们有一枚可能不公平的硬币，我们想要估计这枚硬币正面朝上的概率 $\theta$。
  - 我们投掷硬币多次，并记录结果。假设我们投掷了10次，得到了8次正面（Head）和2次反面（Tail）。
  - **最大化似然**的目标就是找到一个 $\theta$ 的值，使得似然函数 $L(θ)$ 的值最大。直观上，这个值应该接近我们观察到的正面概率，即8/10或0.8。
- variational Lower Bound
  - 变分（variational）
    - 一词通常与使用优化技术来近似难以直接处理的复杂概率分布有关。这个词来源于变分法，它是一种寻找函数极值的方法，尤其是在概率模型中，当我们试图**最大化或最小化某个函数（如似然函数）**时。
  - “下界”（Lower Bound）
    - 指对于给定的优化问题，可以确定的一个值，这个值小于或等于问题的真实最优解。
      - 但在更复杂的情况下，比如我们不能确定每次摸到的球的颜色，或者球的颜色会随时间变化，直接计算似然就变得不可行。这时，我们可以使用变分方法来近似似然。我们选择一个简化的模型，比如假设摸球过程中有一定的错误率，然后尝试**找到最佳的参数 $ϕ$ 来描述实际的红球比例。**
    - 例子中，“变分下界”就是我们通过调整 $ϕ$ 找到的简化模型对 $θ$ 的最佳估计，而这个估计给出的似然值是真实似然的一个保守估计或“下界”。
      - 这意味着，根据我们的简化模型，摸到的红球次数出现的概率至少是这个值
- "Likelihood-based generative models"
  - are a class of statistical models used for **generating new data points that have a high probability of belonging to the same distribution as a given dataset**.
  - In other words, these models are trained to generate data that **resemble the real data as closely as possible in terms of statistical properties**.
- "the forward process"
  - refers to the sequence of steps that **gradually adds noise to the data until it becomes a sample from a simple distribution** 
- The Gaussian noise
  - The Gaussian noise itself **is not fixed**; it is randomly sampled at each step of the forward process. However, **the variance of the Gaussian noise at each step is fixed** according to a predetermined noise schedule.
- "the forward process is fixed"
  - **the method** of adding noise to the original data to create the series of increasingly noisy data points $x_t$​ (up to $x_T$​) **is predefined and does not change** during training.

- Cross-Attention:
  - Cross-attention extends this concept by allowing the model to **attend to one set of inputs based on another set of inputs**. Essentially, it's about **correlating information across two different sources**.
  -  Cross-attention is particularly powerful in tasks where the **output generation is closely tied to the input, but the relationship is not simply linear or sequential**, such as in translation, summarization, or conditional image generation.


