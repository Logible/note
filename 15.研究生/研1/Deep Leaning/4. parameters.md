# 4

- [4](#4)
  - [Train/dev/test sets](#traindevtest-sets)
  - [Bias/Variance](#biasvariance)
  - [Basic "recipe" for machine learning](#basic-recipe-for-machine-learning)
  - [Regularization](#regularization)
  - [Why regularization reduces overfitting](#why-regularization-reduces-overfitting)
  - [Dropout regularization](#dropout-regularization)
  - [how Dropout work](#how-dropout-work)
  - [Other regularization methods](#other-regularization-methods)
  - [Normalizing inputs](#normalizing-inputs)
  - [Vanishing/exploding gradients](#vanishingexploding-gradients)
  - [Weight initialization for deep networks](#weight-initialization-for-deep-networks)
  - [Numerical approximation of gradients](#numerical-approximation-of-gradients)
  - [Gradient Checking](#gradient-checking)

## Train/dev/test sets

- Applied ML is a highly iterative process
  - ![Alt text](images/image-84.png)

## Bias/Variance

- Bias/Variance
  - ![Alt text](images/image-87.png)
  - ![Alt text](images/image-88.png)

## Basic "recipe" for machine learning

- ![Alt text](images/image-89.png)

## Regularization

- Logistic regression
  - ![Alt text](images/image-90.png)
- Neural network
  - ![Alt text](images/image-91.png)

## Why regularization reduces overfitting

- How does regularization prevent overfitting
  - ![Alt text](images/image-92.png)
- And so the activation function if is tanh, will be **relatively linear**.
  - ![Alt text](images/image-93.png)

## Dropout regularization

- set some probability of eliminating a node in neural network
  - ![Alt text](images/image-213.png)
- on different process through the training set, you should randomly zero out different hidden units
  - Inverted dropout
    - ![Alt text](images/image-214.png)
- At the test time you're not using dropout

## how Dropout work

- the effect of implementing dropout is that it shrinks the weights
- even more adaptive to the scale of different inputs
  - ![Alt text](images/image-211.png)

## Other regularization methods

- Data augmentation
  - ![Alt text](images/image-94.png)
- Early stopping
  - ![Alt text](images/image-95.png)
  - without needing to try a lot of values of the L2 regularization hyperparameter lambda.![Alt text](images/image-96.png)

## Normalizing inputs

1. subtract mean
2. Normalize the variances 

- scale your test set in exactly the same way
  - ![Alt text](images/image-97.png)
- why
  - ![Alt text](images/image-98.png)

## Vanishing/exploding gradients

- 当梯度爆炸发生时，权重更新的幅度非常大。这意味着每次迭代都会导致模型参数的巨大变化，而不是渐进式的优化。这样的情况下，算法很难找到合适的参数值，因为参数值的摆动范围太大。
  - ![Alt text](images/image-99.png)

## Weight initialization for deep networks

- ⭐高斯？
  - ![Alt text](images/image-100.png)
- a reasonable scaling
  - ![Alt text](images/image-101.png)

## Numerical approximation of gradients

- ![Alt text](images/image-102.png)

## Gradient Checking

- ![Alt text](images/image-103.png)
- ![Alt text](images/image-105.png)
  - 欧几里得范数![Alt text](images/image-104.png)

- Gradient Checking implementation notes
  - ![Alt text](images/image-106.png)
