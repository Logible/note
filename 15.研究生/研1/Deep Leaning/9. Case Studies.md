# Case Studies

- [Case Studies](#case-studies)
  - [Outline](#outline)
  - [Classic networks](#classic-networks)
  - [Residual Networks (ResNets)](#residual-networks-resnets)
  - [Why ResNets work](#why-resnets-work)
  - [Network in Network and 1x1 convolutions](#network-in-network-and-1x1-convolutions)
  - [Inception network motivation](#inception-network-motivation)
  - [Inception network](#inception-network)
  - [Using open-source implementations](#using-open-source-implementations)
  - [Transfer Learning](#transfer-learning)
  - [Data augmentation](#data-augmentation)
  - [The state of computer vision](#the-state-of-computer-vision)

## Outline

- ![Alt text](images/image-186.png)

## Classic networks

- LeNet - 5（3
  - ![Alt text](images/image-187.png)
- AlexNet（1
  - ![Alt text](images/image-188.png)
- VGG - 16（2
  - ![Alt text](images/image-189.png)

## Residual Networks (ResNets)

- Residual block(skip connection(use Relu)
  - ![Alt text](images/image-190.png)
  - ![Alt text](images/image-191.png)

## Why ResNets work

- it's so easy for these extra layers **to\no to** learn the identity function, that you are kind of guaranteed that it doesn't hurt performance
- $z^l+2$ and $a^l$ have the same dimension
  - 当wx+b>0时，通常神经元会被激活，产生一个非零的输出，表示神经元"学到了东西"，即它对输入数据中的某些模式或特征产生了响应。
  - ![Alt text](images/image-192.png)
- ![Alt text](images/image-193.png)

## Network in Network and 1x1 convolutions

- Network in Network
  - ![Alt text](images/image-194.png)
- Using 1x1 convolutions
  - ![Alt text](images/image-195.png)
  - 

## Inception network motivation

- Motivation for inception network
  - ![Alt text](images/image-196.png)
- The problem of computational cost
  - ![Alt text](images/image-197.png)
- Bottle neck layer
  - ![Alt text](images/image-198.png)

## Inception network

- Inception module
  - ![Alt text](images/image-199.png)
- Inception network
  - ![Alt text](images/image-200.png)

## Using open-source implementations

## Transfer Learning

- Transfer Learning
  - ![Alt text](images/image-201.png)

## Data augmentation

- Common augmentation method
  - ![Alt text](images/image-202.png)
- Color shifting
  - ![](images/image-203.png)
-  run in parallel
   -  ![Alt text](images/image-204.png)

## The state of computer vision

- Data vs.hand-engineering
  - ![Alt text](images/image-205.png)
- Tips for doing well on benchmarks/winning competitions
  - ![Alt text](images/image-206.png)
- Use open source code
  - ![Alt text](images/image-207.png)