## batch

- [Mini-batch gradient descent](#mini-batch-gradient-descent)
  - [Understanding mini-batch gradient descent](#understanding-mini-batch-gradient-descent)
  - [Exponentially weighted averages](#exponentially-weighted-averages)
  - [Bias correction in exponentially weighted average](#bias-correction-in-exponentially-weighted-average)
  - [Gradient descentwith momentum](#gradient-descentwith-momentum)
  - [RMSprop(Root Mean Square prop)](#rmsproproot-mean-square-prop)
  - [Adam optimization algorithm](#adam-optimization-algorithm)
  - [Learning rate decay](#learning-rate-decay)
  - [The problem of local optima](#the-problem-of-local-optima)

# Mini-batch gradient descent

- ![Alt text](images/image-107.png)
- ![Alt text](images/image-108.png)

## Understanding mini-batch gradient descent

- Training with mini batch gradient descent![Alt text](images/image-109.png)

- One is that you do get a lot of vectorization
- make progress without needing to wait till you process the entire training set
  - ![Alt text](images/image-110.png)

## Exponentially weighted averages

- 0.98 绿线; 0.9 红线; 0.5 黄线
  - ![Alt text](images/image-111.png)
- ![Alt text](images/image-112.png)
- ![Alt text](images/image-113.png)

## Bias correction in exponentially weighted average

- ![Alt text](images/image-114.png)

## Gradient descentwith momentum

- Gradient descent example
  - ![Alt text](images/image-115.png)
- But both versions having Beta equal 0.9 is a common choice of hyperparameter
  - ![Alt text](images/image-116.png)

## RMSprop(Root Mean Square prop)

- allowing you to maybe use a larger learning rate alpha, And certainly speeding up the learning speed of your algorithm.
  - ![Alt text](images/image-117.png)

## Adam optimization algorithm

- ![Alt text](images/image-118.png)
- Hyperparameters choice![Alt text](images/image-119.png)

## Learning rate decay

- ![Alt text](images/image-120.png)
- exponential decay and so on
  - ![Alt text](images/image-121.png)

## The problem of local optima

- Local optima in neural networks![Alt text](images/image-122.png)
- It turns out that plateaus can really slow down learning![Alt text](images/image-123.png)