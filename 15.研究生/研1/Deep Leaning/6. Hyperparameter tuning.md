# Hyperparameter tuning

- [Hyperparameter tuning](#hyperparameter-tuning)
  - [Tuning Process](#tuning-process)
  - [Using an appropriatescale to pickhyperparameters](#using-an-appropriatescale-to-pickhyperparameters)
  - [Hyperparameters tuning in practice: Pandas vs. Caviar](#hyperparameters-tuning-in-practice-pandas-vs-caviar)
  - [Batch Normalization](#batch-normalization)
    - [Normalizing activationsin a network](#normalizing-activationsin-a-network)
    - [Fitting Batch Norminto a neural network](#fitting-batch-norminto-a-neural-network)
  - [Why doesBatch Norm work](#why-doesbatch-norm-work)
  - [Batch Norm at test time](#batch-norm-at-test-time)
  - [Softmax regression](#softmax-regression)
  - [Training a softmax classifier](#training-a-softmax-classifier)

## Tuning Process

- Important![Alt text](images/image-124.png)
- Try random values![Alt text](images/image-125.png)
- Coarse to fine![Alt text](images/image-126.png)

## Using an appropriatescale to pickhyperparameters

- ![Alt text](images/image-127.png)
- Appropriate scale for hyperparameters![Alt text](images/image-128.png)
- Hyperparameters for exponentially weighted averages![Alt text](images/image-129.png)

## Hyperparameters tuning in practice: Pandas vs. Caviar

- ![Alt text](images/image-130.png)

## Batch Normalization

### Normalizing activationsin a network

- Normalize Z![Alt text](images/image-131.png)
- So every component of Z has mean zero and variance one
- this allows you to make the hidden unit values of other **means and variances** as well.
  - ![Alt text](images/image-132.png)

### Fitting Batch Norminto a neural network

- Adding Batch Norm to a network![Alt text](images/image-133.png)
- Working with mini-batches
  - you have got to get rid of b
  - ![Alt text](images/image-134.png)
- Implementing gradient descent

## Why doesBatch Norm work

- it limits the amount to which updating the parameters in the earlier layers
  - ![Alt text](images/image-135.png)
- Batch Norm as regularization![Alt text](images/image-136.png)

## Batch Norm at test time

- Batch Norm at test time![Alt text](images/image-137.png)

## Softmax regression

- ![Alt text](images/image-138.png)
- it needs to take in a vector of input and then outputs a vector![Alt text](images/image-139.png)
- ![Alt text](images/image-140.png)
 
## Training a softmax classifier

- hardmax![Alt text](images/image-141.png)
- Loss function- yes![Alt text](images/image-142.png)
- Gradient descent with softmax![Alt text](images/image-143.png)