# Hyperparameter tuning

- [Hyperparameter tuning](#hyperparameter-tuning)
  - [Tuning Process](#tuning-process)
  - [Using an appropriate scale to pick hyperparameters](#using-an-appropriate-scale-to-pick-hyperparameters)
  - [Hyperparameters tuning in practice: Pandas vs. Caviar](#hyperparameters-tuning-in-practice-pandas-vs-caviar)
  - [Batch Normalization](#batch-normalization)
    - [Normalizing activationsin a network](#normalizing-activationsin-a-network)
    - [Fitting Batch Norm into a neural network](#fitting-batch-norm-into-a-neural-network)
  - [Why does Batch Norm work](#why-does-batch-norm-work)
  - [Batch Norm at test time](#batch-norm-at-test-time)
  - [Softmax regression](#softmax-regression)
  - [Training a softmax classifier](#training-a-softmax-classifier)

## Tuning Process

- Important
  - ![Alt text](images/image-124.png)
- Try random values
  - ![Alt text](images/image-125.png)
- Coarse to fine
  - ![Alt text](images/image-126.png)

## Using an appropriate scale to pick hyperparameters

- ![Alt text](images/image-127.png)
- Appropriate scale for hyperparameters
  - ![Alt text](images/image-128.png)
- Hyperparameters for exponentially weighted averages
  - ![Alt text](images/image-129.png)

## Hyperparameters tuning in practice: Pandas vs. Caviar

- ![Alt text](images/image-130.png)

## Batch Normalization

### Normalizing activationsin a network

- Normalize Z
  - So to train w^3,b^3 faster
  - ![Alt text](images/image-131.png)
- So every component of Z has mean zero and variance one
- this allows you to make the hidden unit values of other **means and variances** as well.
  - ![Alt text](images/image-132.png)

### Fitting Batch Norm into a neural network

- Adding Batch Norm to a network
  - ![Alt text](images/image-133.png)
- Working with mini-batches
  - you have got to get rid of b
  - ![Alt text](images/image-134.png)
- Implementing gradient descent

## Why does Batch Norm work

- it limits the amount to which updating the parameters in the earlier layers
  - During training, the distribution of inputs to each layer can change as the parameters of the preceding layers are updated. This phenomenon is known as internal covariate shift. Batch Norm reduces internal covariate shift by normalizing the inputs within each mini-batch. Normalizing the inputs helps maintain a more consistent distribution of activations throughout the network, making it easier for the model to learn and converge.
  - ![Alt text](images/image-135.png)
- Batch Norm as regularization
  - ![Alt text](images/image-136.png)

## Batch Norm at test time

- Batch Norm at test time
  - But in practice, what people usually do is implement an exponentially weighted average
  - ![Alt text](images/image-137.png)

## Softmax regression

- ![Alt text](images/image-138.png)
- it needs to take in a vector of input and then outputs a vector![Alt text](images/image-139.png)
- ![Alt text](images/image-140.png)
 
## Training a softmax classifier

- hard max
  - ![Alt text](images/image-141.png)
- Loss function- yes
  - ![Alt text](images/image-142.png)
- Gradient descent with softmax
  - ![Alt text](images/image-143.png)