# 3. Neural Network

- What is a Neural Network?![Alt text](images/image-51.png)
- ![Alt text](images/image-52.png)

## Neural Network Representation

- ![Alt text](images/image-53.png)
- ![Alt text](images/image-54.png)

## Vectorizing acrossmultiple examples

- So the **horizontal**, the matrix A goes over our different training examples.
- and vertically, the different **indices** in the matrix A, correspond to different hidden layer.
- ![Alt text](images/image-55.png)

## Explanation for vectorized implementation

- Justification for vectorized implementation![Alt text](images/image-56.png)
- Recap of vectorizing across multiple examples![Alt text](images/image-57.png)

## Activation functions

- and the advantage of both the ReLU and the leaky ReLU is that for a lot of the space of z the derivative of the activation function the slope of the activation function is **very different from zero**
- ![Alt text](images/image-58.png)

## Why does neural network need a nonlinear activation function

- ![Alt text](images/image-59.png)

## Derivatives of activation functions

- Sigmoid activation function
  - ![Alt text](images/image-60.png)
- tanh
  - ![Alt text](images/image-61.png)
- Relu
  - ![Alt text](images/image-62.png)

## Gradient descent for neural networks

- ![Alt text](images/image-63.png)
- back⭐⭐![Alt text](images/image-64.png)

## Random lnitialization