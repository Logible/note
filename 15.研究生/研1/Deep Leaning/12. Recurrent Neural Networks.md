# sequence data

- [sequence data](#sequence-data)
  - [Why sequence models?](#why-sequence-models)
  - [Notation](#notation)
  - [Recurrent Neural Network Model](#recurrent-neural-network-model)
  - [Back propagation through time](#back-propagation-through-time)
  - [Different types of RNNs](#different-types-of-rnns)
  - [Language model and sequence generation](#language-model-and-sequence-generation)
  - [Sampling novel sequences](#sampling-novel-sequences)
  - [Vanishing gradients with RNNs](#vanishing-gradients-with-rnns)
  - [Gated Recurrent Unit (GRU)](#gated-recurrent-unit-gru)
  - [LSTM (Long Short Term Memory) unit](#lstm-long-short-term-memory-unit)
  - [Bidirectional RNN](#bidirectional-rnn)
  - [Deep RNNs](#deep-rnns)

## Why sequence models?

- Examples of sequence data
  - ![Alt text](images/image-259-1.png)

## Notation

- Motivating example
  - ![Alt text](images/image-252.png)
- Representing words
  - ![Alt text](images/image-253.png)

## Recurrent Neural Network Model

- why not a standard network
  - ![Alt text](images/image-254.png)
- Recurrent Neural Networks
  - ![Alt text](images/image-255.png)
- Forward Propagation
  - ![Alt text](images/image-256.png)
- Simplified RNN notation
  - ![Alt text](images/image-258.png)

- 行表示当前隐藏状态的维度, 列表示前一个隐藏状态的维度
  - ![Alt text](images/image-257.png)

## Back propagation through time

- Forward propagation and backpropagation
  - ![Alt text](images/image-259.png)

## Different types of RNNs

- Examples of RNN architectures
  - ![Alt text](images/image-260.png)
  - ![Alt text](images/image-261.png)
- Summary of RNN types
  - ![Alt text](images/image-262.png)

## Language model and sequence generation

- What is language modelling
  - ![Alt text](images/image-263.png)
- Language modelling with an RNN
  - Tokenize
  - ![Alt text](images/image-264.png)
- RNN model
  - So each step in the RNN, will look at some set of preceding words, such as, given the first three words, what is the distribution over the next word? And so this RNN learns to predict one word at a time going from left to right.
  - ![Alt text](images/image-265.png)

## Sampling novel sequences

- one of the ways you can informally get a sense of **what is learned** is to have a **sample novel sequences**
  - np.random.choice 是 NumPy 提供的用于从给定数组中进行随机抽样的函数
  - ![Alt text](images/image-266.png)
- Character-level language model
  - ![Alt text](images/image-267.png)
- Sequence generation
  - ![Alt text](images/image-268.png)

## Vanishing gradients with RNNs

- Vanishing gradients with RNNs is not be very good at capturing long-range dependencies
  - re-scale some of your gradient vector so that is not too big
  - ![Alt text](images/image-269.png)

## Gated Recurrent Unit (GRU)

- RNN unit
  - ![Alt text](images/image-270.png)
- GRU(simplified)
  - ![Alt text](images/image-271.png)
- So you can choose to keep **some bits constant** while updating other bits.
  - ![Alt text](images/image-272.png)
- Full GRU
  - ![Alt text](images/image-273.png)

## LSTM (Long Short Term Memory) unit

- GRU and LSTM
  - ![Alt text](images/image-274.png)
- LSTM in pictures
  - ![Alt text](images/image-275.png)

## Bidirectional RNN

- Getting information from the future
  - ![Alt text](images/image-276.png)
- Bidirectional RNN(BRNN)
  - ![Alt text](images/image-277.png)

## Deep RNNs

- Deep RNN example(For learning very complex functions)
  - ![Alt text](images/image-278.png)