# Basics of Neura Network Programming

- [Basics of Neura Network Programming](#basics-of-neura-network-programming)
  - [Binary Classification](#binary-classification)
  - [Logistic Regression](#logistic-regression)
  - [Logistic Regression cost function](#logistic-regression-cost-function)
  - [Computing derivatives](#computing-derivatives)
  - [Logistic regression derivatives](#logistic-regression-derivatives)
  - [vectorization](#vectorization)
  - [Broadcasting in Python](#broadcasting-in-python)
  - [A note on python](#a-note-on-python)
  - [Logistic regression cost function(optional)](#logistic-regression-cost-functionoptional)

## Binary Classification

- dimension![Alt text](images/image-2.png)
- Notation![Alt text](images/image-3.png)

## Logistic Regression

- ![Alt text](images/image-4.png)

## Logistic Regression cost function

- ![Alt text](images/image-5.png)

## Computing derivatives

- ![Alt text](images/image-6.png)

## Logistic regression derivatives

- ![Alt text](images/image-7.png)
- Logistic regression on m examples
  - ![Alt text](images/image-72.png)

## vectorization

- What is vectorization?
  - ![Alt text](images/image-9.png)
- Vectors and matrix valued functions
  - ![Alt text](images/image-10.png)
- Logistic regression derivatives
  - ![Alt text](images/image-11.png)
  - dw = np.zeros((n_x, 1))

```py
dw = np.zeros((n_x, 1))
dw += x^i*(dz)^i # x^i表示x的第i个特征
dw/m
# 为什么上述代码就可以计算dw梯度
# 因为每次都会把对应的w_i加到对应的行上
```

- Vectorizing Logistic Regression
  - ![Alt text](images/image-12.png)
  - ![Alt text](images/image-14.png)
  - ![Alt text](images/image-13.png)

## Broadcasting in Python

- ![Alt text](images/image-43.png)
- ![Alt text](images/image-44.png)
- ![Alt text](images/image-45.png)

## A note on python

- Not a rank 1 aaray![Alt text](images/image-47.png)

## Logistic regression cost function(optional)

- y hat![Alt text](images/image-48.png)
- Logistic regression cost function![Alt text](images/image-49.png)
- Maximum likelihood estimation
  - which just means to choose the parameters that maximizes this thing
- m example loss function, got to rid of the minus sign.
  - ![Alt text](images/image-50.png)