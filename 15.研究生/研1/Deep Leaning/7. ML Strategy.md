# ML Strategy

- [ML Strategy](#ml-strategy)
  - [Why](#why)
  - [Orthogonalization](#orthogonalization)
  - [Single numbere valuation metric](#single-numbere-valuation-metric)
  - [Satisficing and optimizing metrics](#satisficing-and-optimizing-metrics)
  - [Train/dev/test distributions](#traindevtest-distributions)
  - [Size of dev and test sets](#size-of-dev-and-test-sets)
  - [When to change dev/test sets and metrics](#when-to-change-devtest-sets-and-metrics)
  - [Why human-level performance?](#why-human-level-performance)
  - [Avoidable bias](#avoidable-bias)
  - [Understanding human-level performance](#understanding-human-level-performance)
  - [Surpassing human-level performance](#surpassing-human-level-performance)
  - [Improving your model performance](#improving-your-model-performance)
  - [Error Analysis](#error-analysis)
    - [Carrying out error analysis](#carrying-out-error-analysis)
    - [Cleaning up Incorrectly labeled data](#cleaning-up-incorrectly-labeled-data)
    - [Build your first system quickly then iterate](#build-your-first-system-quickly-then-iterate)
  - [Mismatched training and dev/test data](#mismatched-training-and-devtest-data)
    - [Bias and Variance with mismatched data distributions](#bias-and-variance-with-mismatched-data-distributions)
    - [Addressing data mismatch](#addressing-data-mismatch)
  - [Transfer learning](#transfer-learning)
  - [Learning from multiple tasks](#learning-from-multiple-tasks)
    - [Multi-task learning](#multi-task-learning)
  - [End-to-end deep learning](#end-to-end-deep-learning)
    - [What is end-to-end deep learning](#what-is-end-to-end-deep-learning)
    - [Whether to use end-to-end learning](#whether-to-use-end-to-end-learning)

## Why

![Alt text](images/image-145.png)

## Orthogonalization

- ![Alt text](images/image-146.png)
- ![Alt text](images/image-147.png)

## Single numbere valuation metric

- Using a single number evaluation metric
  - ![Alt text](images/image-148.png)
- average
  - ![Alt text](images/image-149.png)

## Satisficing and optimizing metrics

- optimizing metric
- satisficing metric: running time
  - ![Alt text](images/image-150.png)

## Train/dev/test distributions

- dev/test sets
  - ![Alt text](images/image-151.png)
- Guideline
  - ![Alt text](images/image-152.png)

## Size of dev and test sets

- ![Alt text](images/image-153.png)

## When to change dev/test sets and metrics

- cat dataset example
  - ![Alt text](images/image-154.png)
- Orthogonalization for cat pictures
  - ![Alt text](images/image-155.png)
- ![Alt text](images/image-156.png)

## Why human-level performance?

- ![Alt text](images/image-157.png)
- Why compare to human-level performance![Alt text](images/image-158.png)

## Avoidable bias

- Difference **Bayes error** or approximation of Bayes error and **the training error** to be the avoidable bias.
- You can't actually do better than Bayes error unless you're overfitting
  - ![Alt text](images/image-159.png)

## Understanding human-level performance

- Human-level error as a proxy for Bayes error
  - ![Alt text](images/image-160.png)
- Error analysis example
  - ![Alt text](images/image-161.png)
- Summary of bias/variance with human,level performance
  - ![Alt text](images/image-162.png)

## Surpassing human-level performance

- ![Alt text](images/image-163.png)
- Problems where ML significantly surpasses human-level performance![Alt text](images/image-164.png)

## Improving your model performance

- ![Alt text](images/image-165.png)

## Error Analysis

### Carrying out error analysis

- ![Alt text](images/image-166.png)

### Cleaning up Incorrectly labeled data

- if it doesn't make a significant difference to your ability, to use the dev set to evaluate cost bias
  - ![Alt text](images/image-167.png)
- Correcting incorrect dev/test set examples
  - ![Alt text](images/image-168.png)

### Build your first system quickly then iterate

- Build your first system quicklythen iterate
  - ![Alt text](images/image-169.png)

## Mismatched training and dev/test data

- ![Alt text](images/image-170.png)
- Speech recognition example
  - ![Alt text](images/image-171.png)

### Bias and Variance with mismatched data distributions

- Cat classifier example
  - ![Alt text](images/image-172.png)
- Summary
  - degree of overfitting to dev set
  - ![Alt text](images/image-173.png)
- More general formulation
  - ![Alt text](images/image-174.png)

### Addressing data mismatch

- Addressing data mismatch
  - street numbers
  - ![Alt text](images/image-175.png)
- Artificial data synthesis
  - ![Alt text](images/image-176.png)

## Transfer learning

- what you do is initialize the last layers' weights
- And if you retrain all the parameters in the neural network,
  - then this initial phase of training on image recognition is sometimes called pre-training
  - And then if you are updating all the weights afterwards, then training on the radiology data sometimes that's called fine tuning
- ![Alt text](images/image-177.png)

- When transfer learning makes sense
  - ![Alt text](images/image-178.png)

## Learning from multiple tasks

### Multi-task learning

- Simplified autonomous driving example
  - ![Alt text](images/image-179.png)
- Neural network architecture
  - you would sum only over values of j with a 0 or 1 label
  - ![Alt text](images/image-180.png)
- When multi-task learning makes sense
  - But if you can train a big enough neural network, then multi-task learning, certainly should not or should very rarely hurt performance.
  - ![Alt text](images/image-181.png)

## End-to-end deep learning

### What is end-to-end deep learning

- ![Alt text](images/image-182.png)
- Face recognition![Alt text](images/image-183.png)
- Machine translation![Alt text](images/image-184.png)

### Whether to use end-to-end learning

- motion planning
  - ![Alt text](images/image-185.png)
