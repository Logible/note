# ML Strategy

- [ML Strategy](#ml-strategy)
  - [Why](#why)
  - [Orthogonalization](#orthogonalization)
  - [Single numberevaluation metric](#single-numberevaluation-metric)
  - [Satisficing andoptimizing metrics](#satisficing-andoptimizing-metrics)
  - [Train/dev/test distributions](#traindevtest-distributions)
  - [Size of devand test sets](#size-of-devand-test-sets)
  - [When to change dev/test sets and metrics](#when-to-change-devtest-sets-and-metrics)
  - [Why human-levelperformance?](#why-human-levelperformance)
  - [Avoidable bias](#avoidable-bias)
  - [Understanding human-levelperformance](#understanding-human-levelperformance)
  - [Surpassing human-level performance](#surpassing-human-level-performance)
  - [Improving your model performance](#improving-your-model-performance)
  - [Error Analysis](#error-analysis)
    - [Carrying out error analysis](#carrying-out-error-analysis)
    - [Cleaning up Incorrectly labeled data](#cleaning-up-incorrectly-labeled-data)
    - [Build your first system quickly then iterate](#build-your-first-system-quickly-then-iterate)
  - [Mismatched trainingand dev/test data](#mismatched-trainingand-devtest-data)
    - [Bias and Variance with mismatched data distributions](#bias-and-variance-with-mismatched-data-distributions)
    - [Addressing data mismatch](#addressing-data-mismatch)
  - [Transfer learning](#transfer-learning)
  - [Learning from multiple tasks](#learning-from-multiple-tasks)
    - [Multi-task learning](#multi-task-learning)
  - [End-to-end deep learning](#end-to-end-deep-learning)
    - [What is end-to-end deep learning](#what-is-end-to-end-deep-learning)
    - [Whether to useend-to-end learning](#whether-to-useend-to-end-learning)

## Why

![Alt text](images/image-145.png)

## Orthogonalization

- ![Alt text](images/image-146.png)
- ![Alt text](images/image-147.png)

## Single numberevaluation metric

- Using a single number evaluation metric![Alt text](images/image-148.png)
- average![Alt text](images/image-149.png)

## Satisficing andoptimizing metrics

- optimizing metric
- satisficing metric: running time
  - ![Alt text](images/image-150.png)

## Train/dev/test distributions

- dev/test sets![Alt text](images/image-151.png)
- Guideline![Alt text](images/image-152.png)

## Size of devand test sets

- ![Alt text](images/image-153.png)

## When to change dev/test sets and metrics

- cat dataset example![Alt text](images/image-154.png)
- Orthogonalization for cat pictures![Alt text](images/image-155.png)
- ![Alt text](images/image-156.png)

## Why human-levelperformance?

- ![Alt text](images/image-157.png)
- Why compare to human-level performance![Alt text](images/image-158.png)

## Avoidable bias

- You can't actually do better than Bayes error unless you're overfitting
  - ![Alt text](images/image-159.png)

## Understanding human-levelperformance

- Human-level error as a proxy for Bayes error![Alt text](images/image-160.png)
- Error analysis example![Alt text](images/image-161.png)
- Summary of bias/variance with human,level performance![Alt text](images/image-162.png)

## Surpassing human-level performance

- ![Alt text](images/image-163.png)
- Problems where ML significantly surpasseshuman-level performance![Alt text](images/image-164.png)

## Improving your model performance

- ![Alt text](images/image-165.png)

## Error Analysis

### Carrying out error analysis

- ![Alt text](images/image-166.png)

### Cleaning up Incorrectly labeled data

- if it doesn't make a significant difference to your ability, to use the dev set to evaluate cost bias
  - ![Alt text](images/image-167.png)
- Correcting incorrect dev/test set examples
  - ![Alt text](images/image-168.png)

### Build your first system quickly then iterate

- Build your first system quicklythen iterate
  - ![Alt text](images/image-169.png)

## Mismatched trainingand dev/test data

- ![Alt text](images/image-170.png)
- Speech recognition example
  - ![Alt text](images/image-171.png)

### Bias and Variance with mismatched data distributions

- Cat classifier example![Alt text](images/image-172.png)
- Summary![Alt text](images/image-173.png)
- More general formulation![Alt text](images/image-174.png)

### Addressing data mismatch

- Addressing data mismatch
  - ![Alt text](images/image-175.png)
- Artificial data synthesis
  - ![Alt text](images/image-176.png)

## Transfer learning

- what you do is initialize the last layers' weights
- And if you retrain all the parameters in the neural network,
  - then this initial phase of training on image recognition is sometimes called pre-training
- And then if you are updating all the weights afterwards, then training on the radiology data sometimes that's called fine tuning
- ![Alt text](images/image-177.png)

- When transfer learning makes sense
  - ![Alt text](images/image-178.png)

## Learning from multiple tasks

### Multi-task learning

- Simplified autonomous driving example
  - ![Alt text](images/image-179.png)
- Neural network architecture
  - ![Alt text](images/image-180.png)
- When multi-task learning makes sense
  - ![Alt text](images/image-181.png)

## End-to-end deep learning

### What is end-to-end deep learning

- ![Alt text](images/image-182.png)
- Face recognition![Alt text](images/image-183.png)
- Machine translation![Alt text](images/image-184.png)

### Whether to useend-to-end learning

- ![Alt text](images/image-185.png)