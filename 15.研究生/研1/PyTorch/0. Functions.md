# Functions

- [Functions](#functions)
  - [unsqueeze](#unsqueeze)
  - [tensor\[0\]](#tensor0)

## unsqueeze

```py
x = torch.tensor([1, 2, 3, 4])
print(x)

# unsqueeze 函数在第一个维度 (dim=0) 上插入一个新的维度
# tensor([[1, 2, 3, 4]]):
print(torch.unsqueeze(x, dim=0))

# unsqueeze 函数在第二个维度 (dim=1) 上插入一个新的维度
# tensor([[1],
#        [2],
#        [3],
#        [4]])
print(torch.unsqueeze(x, dim=1))
```

tensor([1, 2, 3, 4]):

这是一个一维张量（向量）。
形状为 (4,)，表示它有**一个维度**，该维度的大小为4。
tensor([[1, 2, 3, 4]]):

这是一个二维张量（矩阵）。
形状为 (1, 4)，表示它有**两个维度**，第一个维度的大小为1，第二个维度的大小为4

## tensor[0]

```py
x = torch.tensor([1, 2, 3, 4])
print(x[1])

A = [[5, 7, 2],
     [1, 8, 6]]

print(A[1])

B = [[[1, 2],
      [3, 4]],

     [[5, 6],
      [7, 8]],

     [[9, 10],
      [11, 12]]]

print(B[1])

# output
tensor(2)
[1, 8, 6]
[[5, 6], [7, 8]]
```

1. 卷积层尺寸 步长 与运算结果的关系
2. 激活函数除了relu还有什么，都有什么不同
3. 梯度更新优化算法 还有哪些
4. softmax、batchnormalization的原理等。