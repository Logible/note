# intruduce 

- [intruduce](#intruduce)
  - [word](#word)
  - [Variable](#variable)
  - [Activation Function 激励函数](#activation-function-激励函数)
  - [regression](#regression)
  - [Classification](#classification)
  - [保持提取](#保持提取)
  - [批训练](#批训练)
  - [Speed up neural network training process](#speed-up-neural-network-training-process)
  - [Optimizer 优化器](#optimizer-优化器)
  - [What is Convolutional Neural Networks](#what-is-convolutional-neural-networks)
  - [CNN](#cnn)

pip install matplotlib -i https://pypi.tuna.tsinghua.edu.cn/simple matplotlib

## word

- back propagation误差反向传递？
  - 误差计算：根据前向传播的输出和实际目标之间的差异，计算损失函数的梯度。这个梯度表示了神经网络预测的误差。

- 张量（tensor）是一个多维数组，类似于矩阵，但可以具有任意数量的维度。张量是这些领域中最基本的数据结构之一，用于存储和表示数据，以及进行各种数学运算。

    下面是一些关于张量的基本概念：

    维度（Rank）： 张量的维度通常被称为"秩"（rank），表示张量的阶数或维度的数量。例如，一个一维张量是一个向量，二维张量是一个矩阵，而三维或更高维度的张量可以包含更多的数据。

        零维张量（0D Tensor）：

            零维张量通常称为标量。它表示单个标量值，例如一个数字。在数学和深度学习中，通常将其表示为一个常数。
            示例：5 或 3.14 是零维张量。
        
        一维张量（1D Tensor）：

            一维张量是一个线性数组，通常称为向量。
            示例：[1, 2, 3, 4] 是一个一维张量。
            shape:(4,)
            
        二维张量（2D Tensor）：

            二维张量是一个矩阵，包含行和列。
            示例：[[1, 2], [3, 4], [5, 6]] 是一个二维张量。
            shape:(3,2)
            
        三维张量（3D Tensor）：

            三维张量是一个立体结构，类似于多个矩阵堆叠在一起。
            示例：[ [ [1, 2], [3, 4] ], [ [5, 6], [7, 8] ] ] 是一个三维张量。
            shape:(2,2,2)


    数据类型（Data Type）： 张量可以包含不同的数据类型，例如整数、浮点数、布尔值等。不同的数据类型适用于不同类型的问题和计算。

    元素（Element）： 张量中的每个数值称为一个元素。元素的值可以存储数据，如图像像素、权重、偏差等

## Variable

```py
import torch
import numpy as np

# 创建数据
np_data = np.arange(6).reshape((2, 3))

# numpy to tensor
torch_data = torch.from_numpy(np_data)
# tensor to numpy
tensor2array = torch_data.numpy()

print('\nnumpy\n', np_data,
      '\ntorch\n', torch_data,
      '\ntensor2array\n', tensor2array)

# abs

# numpy格式
data = [-1, -2, 1, 2]
# 转torch
tensor = torch.FloatTensor(data)  # 32bit

print(np.abs(data))
print(torch.abs(tensor))

# matrix

data = [[1, 2], [3, 4]]
tensor = torch.FloatTensor(data)# 32-bit floating point

print('\nnumpy:', np.matmul(data, data),
      '\ntorch:', torch.mm(tensor, tensor))
```

```py
import torch
from torch.autograd import Variable

# Variable in torch is to build a computational graph,
# but this graph is dynamic compared with a static graph in Tensorflow or Theano.
# So torch does not have placeholder, torch can just pass variable to the computational graph.

tensor = torch.FloatTensor([[1, 2], [3, 4]])  # build a tensor
variable = Variable(tensor, requires_grad=True)  # build a variable, usually for compute gradients

# v_out = 1/4 * sum(variable*variable)
v_out = torch.mean(variable*variable)   # x^2
v_out.backward()  # backpropagation from v_out

# the gradients w.r.t the variable, d(v_out)/d(variable) = 1/4*2*variable = variable/2
# 问题：mean的作用呢？ mean作用就是1/n
print(variable.grad)
print(variable.data)  # tensor形式
print(variable.data.numpy())  # numpy形式
```

## Activation Function 激励函数

```py
import torch
import torch.nn.functional as F
from torch.autograd import Variable
import matplotlib.pyplot as plt

# fake data
x = torch.linspace(-5, 5, 200)  # x data (tensor), shape=(200,)
x = Variable(x)
x_np = x.data.numpy()   # numpy array for plotting

# following are popular activation functions
y_relu = F.relu(x).data.numpy()
y_sigmoid = F.sigmoid(x).data.numpy()
y_tanh = F.tanh(x).data.numpy()
y_softplus = F.softplus(x).data.numpy() # there's no softplus in torch
# y_softmax = torch.softmax(x, dim=0).data.numpy() softmax is a special kind of activation function, it is about probability
```

- ![Alt text](images/image.png)
  - `x = torch.linspace(-5，5，200)`

    这行代码使用 PyTorch 创建了一个张量 x，其中包含了从 -5 到 5 的 200 个等间隔值。具体来说，这行代码执行了以下操作：

    torch.linspace(start, end, steps) 创建一个张量，包含从 start 到 end 之间的 steps 个等间隔的值。
    在你的代码中，start 设置为 -5，end 设置为 5，steps 设置为 200。
    因此，x 包含了从 -5 到 5 之间的 200 个等间隔值，用于表示一个连续的数值范围。

- (200,) 表示一个包含200个元素的一维张量。这是一个扁平的张量，其中的元素**排成一行**，没有额外的维度。这种形状通常用于表示一维数据或向量。
  - (200, 1) 表示一个包含200个元素的二维张量，其中有一个维度为1。这可以被看作是一个**列向量，每个元素位于不同的行**，而列数为1。这种形状通常用于表示多个单变量数据点或作为神经网络的输入。

```py
x = torch.linspace(1, 10, 10)
print(x)
tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])
```

## regression

```py
"""
View more, visit my tutorial page: https://mofanpy.com/tutorials/
My Youtube Channel: https://www.youtube.com/user/MorvanZhou

Dependencies:
torch: 0.4
matplotlib
"""
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt

# torch.manual_seed(1)    # reproducible

# 使用 torch.unsqueeze(..., dim=1) 用于维度扩展的主要目的是将原始张量的形状从一维张量（一维数组）变为二维张量，其中的值排列在一个列向量中
x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)  # x data (tensor), shape=(100, 1)
y = x.pow(2) + 0.2*torch.rand(x.size())                 # noisy y data (tensor), shape=(100, 1)

# torch can only train on Variable, so convert them to Variable
# The code below is deprecated in Pytorch 0.4. Now, autograd directly supports tensors
# x, y = Variable(x), Variable(y)

# plt.scatter(x.data.numpy(), y.data.numpy())
# plt.show()

# 这行代码定义了一个名为 Net 的类，它继承自 torch.nn.Module，这是 PyTorch 中用于构建神经网络模型的基类。
class Net(torch.nn.Module):
    def __init__(self, n_feature, n_hidden, n_output):
        # super(Net, self).__init__()：这一行调用父类 torch.nn.Module 的构造函数，确保正确初始化神经网络。
        super(Net, self).__init__()

        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer
        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer

    def forward(self, x):
        x = F.relu(self.hidden(x))      # activation function for hidden layer
        x = self.predict(x)             # linear output
        return x

net = Net(n_feature=1, n_hidden=10, n_output=1)     # define the network
print(net)  # net architecture

# net.parameters() 返回神经网络模型 net 中的所有可学习参数，这些参数将在训练过程中更新。
# 一开始的值是什么, 高斯初始化参数
optimizer = torch.optim.SGD(net.parameters(), lr=0.2)
loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss

plt.ion()   # something about plotting

for t in range(200):
    prediction = net(x)     # input x and predict based on x

    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)

    optimizer.zero_grad()   # clear gradients for next train
    loss.backward()         # backpropagation, compute gradients
    # 它通常在你计算完梯度后，将这些梯度应用到模型的参数上以更新参数值
    optimizer.step()        # apply gradients

    if t % 5 == 0:
        # plot and show learning process
        plt.cla()
        plt.scatter(x.data.numpy(), y.data.numpy())
        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)
        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})
        plt.pause(0.1)

plt.ioff()
plt.show()
```

- SGD Stochastic Gradient Descent
  - SGD的随机性使得训练更加高效。因为它不需要计算整个训练数据集的梯度，而只需要计算小批量数据的梯度，这通常可以大大加快训练速度。此外，SGD的随机性使得计算梯度的过程可以并行化，因为不同的小批量数据可以在不同的处理单元上计算
- torch.optim
  - PyTorch 中的优化模块，它包含了许多优化算法，可用于训练深度学习模型。这个模块提供了各种优化器的实现，包括随机梯度下降（SGD）、Adam、Adagrad、RMSprop 等，以及许多变种和改进版本。
- 在深度学习中，梯度的累积是因为采用了自动微分（automatic differentiation）的计算方法。当你对损失函数进行反向传播（计算梯度）时，每次**调用反向传播都会将梯度添加到现有梯度上**，而不是替代它

## Classification

```py
"""
View more, visit my tutorial page: https://mofanpy.com/tutorials/
My Youtube Channel: https://www.youtube.com/user/MorvanZhou

Dependencies:
torch: 0.4
matplotlib
"""
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt

# torch.manual_seed(1)    # reproducible

# make fake data
# 创建一个形状为 (100, 2) 的张量，其中所有元素都是1。这个张量用于生成数据。
n_data = torch.ones(100, 2)

# 在代码中，噪声是通过使用 torch.normal(mean, std) 函数来添加的，其中 mean 表示均值，std 表示标准差。在这种情况下，均值为2，标准差为1。
x0 = torch.normal(2*n_data, 1)      # class0 x data (tensor), shape=(100, 2)
y0 = torch.zeros(100)               # class0 y data (tensor), shape=(100,)

x1 = torch.normal(-2*n_data, 1)     # class1 x data (tensor), shape=(100, 2)
y1 = torch.ones(100)                # class1 y data (tensor), shape=(100,)

# 参数中的 0 表示连接的维度，具体来说，它指定了在哪个维度上连接这些张量
# 如果维度参数是0，那么它表示在行的维度上进行连接。
x = torch.cat((x0, x1), 0).type(torch.FloatTensor)  # shape (200, 2) FloatTensor = 32-bit floating
y = torch.cat((y0, y1), ).type(torch.LongTensor)    # shape (200,) LongTensor = 64-bit integer

# The code below is deprecated in Pytorch 0.4. Now, autograd directly supports tensors
# x, y = Variable(x), Variable(y)

# plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=y.data.numpy(), s=100, lw=0, cmap='RdYlGn')
# plt.show()


class Net(torch.nn.Module):
    def __init__(self, n_feature, n_hidden, n_output):
        # super(Net, self).__init__()：这一行调用父类 torch.nn.Module 的构造函数，确保正确初始化神经网络。
        super(Net, self).__init__()
        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer
        self.out = torch.nn.Linear(n_hidden, n_output)   # output layer

    def forward(self, x):
        x = F.relu(self.hidden(x))      # activation function for hidden layer
        x = self.out(x)
        return x

# 问题：为什么调用时都不用指定n_hidden
# 在这个示例中，n_hidden 参数的值不需要在模型的前向传播函数中显式指定，因为它已经在模型的构造函数 __init__ 中指定并用于初始化模型的隐藏层

net = Net(n_feature=2, n_hidden=10, n_output=2)     # define the network
print(net)  # net architecture

optimizer = torch.optim.SGD(net.parameters(), lr=0.02)
loss_func = torch.nn.CrossEntropyLoss()  # the target label is NOT an one-hotted

plt.ion()   # something about plotting

for t in range(100):
    out = net(x)                 # input x and predict based on x
    loss = loss_func(out, y)     # must be (1. nn output, 2. target), the target label is NOT one-hotted

    optimizer.zero_grad()   # clear gradients for next train
    loss.backward()         # backpropagation, compute gradients
    optimizer.step()        # apply gradients

    if t % 2 == 0:
        # plot and show learning process
        plt.cla()
        # torch.max(out, 1)为行最大值，()[1]为最大值的索引，()[0]为最大值的值
        prediction = torch.max(out, 1)[1]
        pred_y = prediction.data.numpy()
        target_y = y.data.numpy()
        plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=pred_y, s=100, lw=0, cmap='RdYlGn')
        accuracy = float((pred_y == target_y).astype(int).sum()) / float(target_y.size)
        plt.text(1.5, -4, 'Accuracy=%.2f' % accuracy, fontdict={'size': 20, 'color':  'red'})
        plt.pause(0.1)

plt.ioff()
plt.show()
```

- method2


```py
net1 = net1(n_feature=2, n_hidden=10, n_output=2)  # define the net1work
print(net1)  # net1 architecture

net2 = torch.nn.Sequential(
    torch.nn.Linear(2, 10),
    torch.nn.ReLU(),
    torch.nn.Linear(10, 2)
)

print(net2)

# net1(
#   (hidden): Linear(in_features=2, out_features=10, bias=True)
#   (out): Linear(in_features=10, out_features=2, bias=True)
# )
# Sequential(
#   (0): Linear(in_features=2, out_features=10, bias=True)
#   (1): ReLU()
#   (2): Linear(in_features=10, out_features=2, bias=True)
# )
```


## 保持提取

```py
def save():
    # save net1
    net1 = torch.nn.Sequential(
        torch.nn.Linear(1, 10),
        torch.nn.ReLU(),
        torch.nn.Linear(10, 1)
    )
    optimizer = torch.optim.SGD(net1.parameters(), lr=0.5)
    loss_func = torch.nn.MSELoss()

    for t in range(100):
        prediction = net1(x)
        loss = loss_func(prediction, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # 2 ways to save the net
    torch.save(net1, 'net.pkl')  # save entire net
    torch.save(net1.state_dict(), 'net_params.pkl')   # save only the parameters


def restore_net():
    # restore entire net1 to net2
    net2 = torch.load('net.pkl')
    prediction = net2(x)

def restore_params():
    # restore only the parameters in net1 to net3
    net3 = torch.nn.Sequential(
        torch.nn.Linear(1, 10),
        torch.nn.ReLU(),
        torch.nn.Linear(10, 1)
    )

    # copy net1's parameters into net3
    net3.load_state_dict(torch.load('net_params.pkl'))
    prediction = net3(x)
```

## 批训练

```py
import torch
import torch.utils.data as Data

torch.manual_seed(1)  # reproducible
# 具体来说，torch.manual_seed(1) 设置了 PyTorch 随机数生成器的种子为1。这意味着在接下来的操作中，生成的随机数将会是确定性的

# BATCH_SIZE = 5
BATCH_SIZE = 8

x = torch.linspace(1, 10, 10)  # this is x data (torch tensor)
y = torch.linspace(10, 1, 10)  # this is y data (torch tensor)

torch_dataset = Data.TensorDataset(x, y)

loader = Data.DataLoader(
    dataset=torch_dataset,  # torch TensorDataset format
    batch_size=BATCH_SIZE,  # mini batch size
    shuffle=True,  # random shuffle for training
    # 将启动 3 个子进程来处理数据加载操作
    num_workers=3,  # subprocesses for loading data
)


def show_batch():
    for epoch in range(3):  # train entire dataset 3 times
        for step, (batch_x, batch_y) in enumerate(loader):  # for each training step
            # train your data...
            print('Epoch: ', epoch, '| Step: ', step, '| batch x: ',
                  batch_x.numpy(), '| batch y: ', batch_y.numpy())


if __name__ == '__main__':
    show_batch()

```

## Speed up neural network training process

- Moment![Moment](images/image-1.png)
- AdaGrad![AdaGrad](images/image-2.png)
- RMSProp![RMSProp](images/image-3.png)
- Adam![Adam](images/image-4.png)

## Optimizer 优化器

```py
import torch
import torch.utils.data as Data
import torch.nn.functional as F
import matplotlib.pyplot as plt

# torch.manual_seed(1)    # reproducible

LR = 0.01
BATCH_SIZE = 32
EPOCH = 12

# fake dataset
x = torch.unsqueeze(torch.linspace(-1, 1, 1000), dim=1)
y = x.pow(2) + 0.1*torch.normal(torch.zeros(*x.size()))

# plot dataset
plt.scatter(x.numpy(), y.numpy())
plt.show()

# put dateset into torch dataset
torch_dataset = Data.TensorDataset(x, y)
loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)


# default network
class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.hidden = torch.nn.Linear(1, 20)   # hidden layer
        self.predict = torch.nn.Linear(20, 1)   # output layer

    def forward(self, x):
        x = F.relu(self.hidden(x))      # activation function for hidden layer
        x = self.predict(x)             # linear output
        return x

if __name__ == '__main__':
    # different nets
    net_SGD         = Net()
    net_Momentum    = Net()
    net_RMSprop     = Net()
    net_Adam        = Net()
    nets = [net_SGD, net_Momentum, net_RMSprop, net_Adam]

    # different optimizers
    opt_SGD         = torch.optim.SGD(net_SGD.parameters(), lr=LR)
    opt_Momentum    = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.8)
    opt_RMSprop     = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)
    opt_Adam        = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9, 0.99))
    optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]

    loss_func = torch.nn.MSELoss()
    losses_his = [[], [], [], []]   # record loss

    # training
    for epoch in range(EPOCH):
        print('Epoch: ', epoch)
        for step, (b_x, b_y) in enumerate(loader):          # for each training step
            for net, opt, l_his in zip(nets, optimizers, losses_his):
                output = net(b_x)              # get output for every net
                loss = loss_func(output, b_y)  # compute loss for every net
                opt.zero_grad()                # clear gradients for next train
                loss.backward()                # backpropagation, compute gradients
                opt.step()                     # apply gradients
                l_his.append(loss.data.numpy())     # loss recoder
```

## What is Convolutional Neural Networks

## CNN

- unsqueeze

```py
x = torch.tensor([1, 2, 3, 4])
print(x)
# unsqueeze 函数在第一个维度 (dim=0) 上插入一个新的维度
print(torch.unsqueeze(x, dim=0))
# unsqueeze 函数在第二个维度 (dim=1) 上插入一个新的维度
print(torch.unsqueeze(x, dim=1))
```

```py
# library
# standard library
import os

# third-party library
import torch
import torch.nn as nn
import torch.utils.data as Data
import torchvision
import matplotlib.pyplot as plt

# torch.manual_seed(1)    # reproducible

# Hyper Parameters
EPOCH = 1  # train the training data n times, to save time, we just train 1 epoch
BATCH_SIZE = 50
LR = 0.001  # learning rate
DOWNLOAD_MNIST = False

# 自动下载 MNIST 数据集，以确保你有数据可供训练和测试你的机器学习模型
# Mnist digits dataset
if not (os.path.exists('./mnist/')) or not os.listdir('./mnist/'):
    # not mnist dir or mnist is empyt dir
    DOWNLOAD_MNIST = True

train_data = torchvision.datasets.MNIST(
    root='./mnist/',
    train=True,  # this is training data
    # 将输入的图像数据（通常是PIL图像或NumPy数组）转换为 PyTorch 张量。
    # 将像素值的范围从 [0, 255]（整数值）缩放到 [0, 1]（浮点数值）。这是因为 PyTorch 张量通常使用浮点数表示，并且像素值在 [0, 1] 范围内更容易处理。
    # 使用 transforms.ToTensor() 是在处理图像数据时的一种标准化操作，以便将图像数据转换为适合输入神经网络的格式。这对于计算机视觉任务中的图像分类、物体检测和图像生成等任务非常有用。
    transform=torchvision.transforms.ToTensor(),  # Converts a PIL.Image or numpy.ndarray to
    
    # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]
    download=DOWNLOAD_MNIST,
)

# plot one example
print(train_data.train_data.size())  # (60000, 28, 28)
print(train_data.train_labels.size())  # (60000)
plt.imshow(train_data.train_data[0].numpy(), cmap='gray')
plt.title('%i' % train_data.train_labels[0])
plt.show()

# Data Loader for easy mini-batch return in training, the image batch shape will be (50, 1, 28, 28)
train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)

# pick 2000 samples to speed up testing
test_data = torchvision.datasets.MNIST(root='./mnist/', train=False)
test_x = torch.unsqueeze(test_data.test_data, dim=1).type(torch.FloatTensor)[:2000] / 255.  # shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1), 其中 1 表示图像通道数
test_y = test_data.test_labels[:2000]


class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Sequential(  # input shape (1, 28, 28)
            nn.Conv2d(
                in_channels=1,  # input height
                out_channels=16,  # n_filters
                kernel_size=5,  # filter size
                stride=1,  # filter movement/step
                padding=2,
                # if want same width and length of this image after Conv2d, padding=(kernel_size-1)/2 if stride=1
            ),  # output shape (16, 28, 28)
            nn.ReLU(),  # activation
            nn.MaxPool2d(kernel_size=2),  # choose max value in 2x2 area, output shape (16, 14, 14)
        )
        self.conv2 = nn.Sequential(  # input shape (16, 14, 14)
            nn.Conv2d(16, 32, 5, 1, 2),  # output shape (32, 14, 14)
            nn.ReLU(),  # activation
            nn.MaxPool2d(2),  # output shape (32, 7, 7)
        )
        self.out = nn.Linear(32 * 7 * 7, 10)  # fully connected layer, output 10 classes

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        # -1 用于自动计算第二维的大小，以确保总的元素数量不变
        # .view() 方法来改变张量的形状
        x = x.view(x.size(0), -1)  # flatten the output of conv2 to (batch_size, 32 * 7 * 7)
        output = self.out(x)
        return output, x  # return x for visualization


cnn = CNN()
print(cnn)  # net architecture

optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)  # optimize all cnn parameters
loss_func = nn.CrossEntropyLoss()  # the target label is not one-hotted

# following function (plot_with_labels) is for visualization, can be ignored if not interested
from matplotlib import cm

try:
    from sklearn.manifold import TSNE; HAS_SK = True
except:
    HAS_SK = False; print('Please install sklearn for layer visualization')


def plot_with_labels(lowDWeights, labels):
    plt.cla()
    X, Y = lowDWeights[:, 0], lowDWeights[:, 1]
    for x, y, s in zip(X, Y, labels):
        c = cm.rainbow(int(255 * s / 9));
        plt.text(x, y, s, backgroundcolor=c, fontsize=9)
    plt.xlim(X.min(), X.max());
    plt.ylim(Y.min(), Y.max());
    plt.title('Visualize last layer');
    plt.show();
    plt.pause(0.01)


plt.ion()
# training and testing
# b_x = {Tensor: (50, 1, 28, 28)}
for epoch in range(EPOCH):
    for step, (b_x, b_y) in enumerate(train_loader):  # gives batch data, normalize x when iterate train_loader

        output = cnn(b_x)[0]  # cnn output
        # ⭐output和b_y形状不同，如何计算？
        loss = loss_func(output, b_y)  # cross entropy loss
        optimizer.zero_grad()  # clear gradients for this training step
        loss.backward()  # backpropagation, compute gradients
        optimizer.step()  # apply gradients

        if step % 50 == 0:
            test_output, last_layer = cnn(test_x)
            pred_y = torch.max(test_output, 1)[1].data.numpy()
            accuracy = float((pred_y == test_y.data.numpy()).astype(int).sum()) / float(test_y.size(0))
            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy(), '| test accuracy: %.2f' % accuracy)
            if HAS_SK:
                # Visualization of trained flatten layer (T-SNE)
                tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)
                plot_only = 500
                low_dim_embs = tsne.fit_transform(last_layer.data.numpy()[:plot_only, :])
                labels = test_y.numpy()[:plot_only]
                plot_with_labels(low_dim_embs, labels)
plt.ioff()

# print 10 predictions from test data
test_output, _ = cnn(test_x[:10])
# torch.max(test_output, 1) 会返回一个元组，其中包含两个张量：第一个张量包含每个输入样本中的最大值，第二个张量包含对应的索引
pred_y = torch.max(test_output, 1)[1].data.numpy()
print(pred_y, 'prediction number')
print(test_y[:10].numpy(), 'real number')
```
