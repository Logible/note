# 6

- [6](#6)
  - [10-30](#10-30)
  - [10-31](#10-31)
  - [11-01](#11-01)
  - [11-02](#11-02)
  - [11-03](#11-03)

## 10-30

1. in practice, L1 regularization to make your model **sparse**
2. So for **arcane** linear algebra technical reasons
3. So this is why L2 norm regularization is also called weight **decay**
4. But the intuition is that by **cranking** up lambda to be really big
5. because this looks like really **warped** fours
6. **nudge/nʌdʒ/** it to the left to get theta minus epsilon/ˈɛpsɪlɒn/
7. applying machine learning is a highly **empirical/ɪmˈpɪrɪkəl/** process
8. So that's why you get these **oscillations/ˌɒsɪˈleɪʃənz/**
9. And the noisiness can be **ameliorated/əˈmiːliəreɪt/** or can be reduced by just using a smaller learning rate.
10. So, there's just a bit more **latency**

## 10-31

1. So this type of a **coarse to fine** search is also frequently used.
2. I've seen that intuitions do get **stale_1**
3. that is watching performance and patiently **nudging** the learning rate up or down

## 11-01

1. we call it Z **tilde~**
2. the need to retrain your function becomes even more **acute**
3. that performs element-wise **exponentiation**
4. lt makes it relatively **interpretable**
5. get closer and closer to hitting the **bullseye/bʊlzaɪ/**
6. the point was with the **philosophy** of orthogonalization/ɔː,θɒɡənəlaɪ'zeɪʃən/
7. **whereupon** you might no longer have a good estimate of Bayes error

## 11-02

1. There is one **caveat/ˈkæviæt/** to this
2. maybe even blurrier because it's shot by **amateur/æmətʃʊr,/** users
3. professionally framed, high **resolution**
4. But you now have a bit of a **dilemma**
5. And for **the sake-of argument**
6. So, in order to **tease out** these two effects
7. very **impoverished** **synthesized/ˈsɪnθɪsaɪz/** data set from
8. both are useful tools to have in your **arsenal**
9. it just lets you through without needing you to carry an RFID **badge**
10. This is a **turnstile** that is giving employees access to a workplace.
11. So machine learning researchers tend to speak **disparagingly** of hand designing things

## 11-03

1. doing well on the training set is usually a **prerequisite/priːˈrekwəzət/** to doing well on your **hold out**
2. Local warping
3. in case you think I'm being **derogatory** of hand engineering
4. and just have a neural network you know **annotate** key positions in the person's pose as well
