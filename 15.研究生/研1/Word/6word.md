# 6

## 10-30

1. because this looks like really **warped** fours
2. **nudge** it to the left to get theta minus epsilon
3. applying machine learning is a highly **empirical** process
4. So that's why you get these **oscillations**
5. And the noisiness can be **ameliorated** or can be reduced by just using a smaller learning rate.
6. So, there's just a bit more **latency**

## 10-31

1. So this type of a **coarse to fine** search is also frequently used.
2. I've seen that intuitions do get **stale**
3. that is watching performance and patiently **nudging** the learning rate up or down

## 11-01

1. we call it Z **tilde~**
2. the need to retrain your function becomes even more **acute**
3. that performs element-wise **exponentiation**
4. lt makes it relatively **interpretable**
5. get closer and closer to hitting the **bullseye**
6. the point was with the **philosophy** of orthogonalization
7. **whereupon** you might no longer have a good estimate of Bayes error