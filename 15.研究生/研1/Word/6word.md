# 6

- [6](#6)
  - [10-30](#10-30)

## 10-30

1. in practice, L1 regularization to make your model **sparse**
2. So for **arcane** linear algebra technical reasons
3. So this is why L2 norm regularization is also called weight **decay**
4. But the intuition is that by **cranking** up lambda to be really big
