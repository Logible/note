# words

## 12-14

1. **multimodal/ˌmʌlti'məudəl/** human–computer interaction is one of the application directions of virtual humans
2. by inputting the corresponding audio and any **mesh/meʃ/ vertex/ˈvɜːteks/**,
3. cross-modal **retrieval/rɪˈtriːvəl/** methods
4. do not consider the **contextual/kənˈtekstʃuəl/** **semantic/sɪˈmæntɪk/** information of the speech.
5. **promoting** the vigorous development of this field
6. We propose two **taxonomies/tækˈsɒnəmi/** to group methods
7. the system pursues multimodal interaction with low-latency and **high-fidelity/fɪˈdeləti/** anthropomorphic virtual humans
8. have multiple **rounds** of dialogue
9. introduced key pose **interpolation**
10. The model can be divided into three types: identity-dependent (D), identity-independent(I), and **hybrid**(H).

## 12-18

1. **Prior to** the popularity of deep learning,
2. many factors, such as prior **assumptions/əˈsʌmpʃən/**
3. shows the **literature** map
4. coefficients
5. Talking-head video generation, **i.e.**, lip motion sequence generation,
6. However, these methods have relatively high requirements on the application environment of the model, visual phoneme annotation, **etc.**
7. used two decoders to **decouple** the voice and the speaker identity to generate the video
8. Meshtalk uses the neutral face **template** mesh as the basis to generate the talking-head video
9. while Meshtalk can **parse out** the absolute latent space of audio-related and audio-independent facial movements.
10. which **simulates** implicit representation with MLP
11. **identity** information coupled with audio features will reduce the accuracy
12. and the **emerging** NeRF
13. a method that is not limited by the input voice and identity is needed.
14. to solve the pixel **jitter** problem in the target area
15. However, in the **generative adversarial network** part of the model
16. AAM coefficients lead to potential errors and limited flexibility **when transferring reference faces to new objects**
17. The 3DMM coefficients include **rigid** head pose parameters, facial recognition coefficients, expression coefficients, **binocular** gaze direction parameters, and **spherical/ˈsferɪkəl/ harmonic illumination/ɪˌluːməˈneɪʃən** coefficients
18. 3D vertex coordinates corresponding to a fixed **topology/təˈpɑlədʒi/** mesh.
19. that **disentangles** audio-related and audio-unrelated information
20. and additional **auxiliary** techniques,
21. are mainly **derived** from the dataset.
22. Si used knowledge **distillation/dɪstɪˈleɪʃn/** to separate emotional,
23. the **displacement** of implicit emotions from audio.
24. a simple audio-visual **synchronization** discriminator
25. an audio-visual **derivative** correlation loss
26. **prosodic** features of speech and lip synchronization
27. address the problem of **incongruity/ˌɪnkənˈɡruːəti/**
28. visual quality and **sensory/ˈsensəri/** effects

- ![Alt text](images/image.png)