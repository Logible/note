# ML

- [ML](#ml)
  - [question](#question)
  - [Superised Learning](#superised-learning)
  - [Unsperised Leaning](#unsperised-leaning)
  - [线性回归(linear regression)](#线性回归linear-regression)
  - [代价函数](#代价函数)
  - [Gradient descent（梯度下降](#gradient-descent梯度下降)
  - [Multiple features(variables)](#multiple-featuresvariables)
  - [Vectorization](#vectorization)
  - [用于多元线性回归的梯度下降法](#用于多元线性回归的梯度下降法)
  - [Feature Scaling](#feature-scaling)
  - [Choosing the Learning Rate](#choosing-the-learning-rate)
  - [Feature Engineering](#feature-engineering)
  - [Polynomial Regression](#polynomial-regression)
  - [Logistic Regression](#logistic-regression)
  - [Decision boundary](#decision-boundary)
  - [Cost function in logistic regression](#cost-function-in-logistic-regression)
  - [simplified Loss function](#simplified-loss-function)
  - [Gradient Descent](#gradient-descent)
  - [The Problem of Overfitting](#the-problem-of-overfitting)
  - [Addressing Overfitting](#addressing-overfitting)
  - [Cost Function with Reqularization](#cost-function-with-reqularization)
  - [Regularized logistic regression](#regularized-logistic-regression)

## question

⭐=⭐

- ![Alt text](images/ee796c8e8bc1ebdbd9bb7740f2ad424.png)
  - yes

## Superised Learning

- the data comes with both inputs x and input lables y

1. Regression
   1. Predict a number **infinitely** many possible outputs
2. Classification
   1. predict categories

## Unsperised Leaning

- Find something interesting in unlabeled data.
- the data comes only with inputs x

1. Clustering
   1. Group similar datapoints together.
2. Dimensionality reduction
   1. Compress data using fewer numbers.
3. Anomaly detection
   1. Find unusual data points.

## 线性回归(linear regression)

- Notation:

  1. x ="input" variable;feature
  2. y ="output" variable;"target" variable
  3. m = number of training examples

1. y-hat: the estimated value of y
2. Model:$f_w,b(x)=wx+b$
   1. w,b:
      1. parameters
      2. cofficients
      3. weight

## 代价函数

![Alt text](images/image.png)

## Gradient descent（梯度下降

![20230925203308](https://raw.githubusercontent.com/Logible/Image/main/note_image/20230925203308.png)

- Alpha, is the **learning rate**.

![20230925204858](https://raw.githubusercontent.com/Logible/Image/main/note_image/20230925204858.png)

- too small or too large

![f5902b286acd6f07ad4e9cb902bdfbf](https://raw.githubusercontent.com/Logible/Image/main/note_image/f5902b286acd6f07ad4e9cb902bdfbf.jpg)

- batch gradient descent

## Multiple features(variables)

- ![ba91bc7423cdeb9678075ba4bd938b7](https://raw.githubusercontent.com/Logible/Image/main/note_image/ba91bc7423cdeb9678075ba4bd938b7.jpg)

- $x_j$ = $j^{th}$ feature
- $n =$ number of featuresn
- $x^{(i)} =$ features of $i^{th}$ training example
- $x_j^{i}$ = value of feature $j$ in $i^{th}$ training example

- multiple linear regression
  - ![20230926153635](https://raw.githubusercontent.com/Logible/Image/main/note_image/20230926153635.png)

## Vectorization

```python
f = np.dot(w,x)+b

w = [w1,w2...wn]
d = [d1,d2...dn]

w = w - 0.1 *d
```

## 用于多元线性回归的梯度下降法

- ![20230926163208](https://raw.githubusercontent.com/Logible/Image/main/note_image/20230926163208.png)

## Feature Scaling

rescaling the different features so they all take on **comparable range of values** can speed up gradient descent significantly

- Mean normalization
  - ![20231002153222](https://raw.githubusercontent.com/Logible/Image/main/note_image/20231002153222.png)
- Z-score normalization
  - ![Alt text](images/image-7.png)

## Choosing the Learning Rate

- leaning curve
  - ![20230926172039](https://raw.githubusercontent.com/Logible/Image/main/note_image/20230926172039.png)

- Debug trick: With a small enough a, J(w, b) should decrease on every iteration
- how to choose
  - pick the **largest possible** learning rate
  - or just something **slightly smaller** than the largest reasonable value

## Feature Engineering

- Using intuition to design new features, by transforming or combining original features.
  - ![1](images/image-1.png)

## Polynomial Regression

- ![20230927161455](https://raw.githubusercontent.com/Logible/Image/main/note_image/20230927161455.png)

## Logistic Regression

- sigmoid function(Logistic regression)
  - $\displaystyle g(z) = \frac{1}{1+e^{-z}}, 0<g(z)<1$
  - ![20230927162142](https://raw.githubusercontent.com/Logible/Image/main/note_image/20230927162142.png)
  - ![20230926193548](https://raw.githubusercontent.com/Logible/Image/main/note_image/20230926193548.png)
- f
  - ![20230926194426](https://raw.githubusercontent.com/Logible/Image/main/note_image/20230926194426.png)

## Decision boundary

- ![20230926200028](https://raw.githubusercontent.com/Logible/Image/main/note_image/20230926200028.png)
- ![Alt text](images/image-2.png)

## Cost function in logistic regression

- logistic Loss function
  - ![Alt text](images/image-3.png)
  - ![20230926204437](https://raw.githubusercontent.com/Logible/Image/main/note_image/20230926204437.png)
  - y = 0![20230926204518](https://raw.githubusercontent.com/Logible/Image/main/note_image/20230926204518.png)
  - y = 1![20230926204535](https://raw.githubusercontent.com/Logible/Image/main/note_image/20230926204535.png)

## simplified Loss function

- ![20230926205031](https://raw.githubusercontent.com/Logible/Image/main/note_image/20230926205031.png)
- ![20230926205427](https://raw.githubusercontent.com/Logible/Image/main/note_image/20230926205427.png)

## Gradient Descent

- ![20230926211043](https://raw.githubusercontent.com/Logible/Image/main/note_image/20230926211043.png)

## The Problem of Overfitting

- ![20230926212353](https://raw.githubusercontent.com/Logible/Image/main/note_image/20230926212353.png)

- over-fit = high-variance
- underfit = high-bias

## Addressing Overfitting

1. Collect more training examples
   1. ![20230927194245](https://raw.githubusercontent.com/Logible/Image/main/note_image/20230927194245.png)
2. Select features to include/exclude
   1. ![20230927194347](https://raw.githubusercontent.com/Logible/Image/main/note_image/20230927194347.png)
3. Regularization
   1. ![20231001214351](https://raw.githubusercontent.com/Logible/Image/main/note_image/20231001214351.png)

## Cost Function with Regularization

- ![20230927203145](https://raw.githubusercontent.com/Logible/Image/main/note_image/20230927203145.png)
- how to choose![20230927203605](https://raw.githubusercontent.com/Logible/Image/main/note_image/20230927203605.png)
- how regularization works![20230927205004](https://raw.githubusercontent.com/Logible/Image/main/note_image/20230927205004.png)

## Regularized logistic regression

- ![20230927210007](https://raw.githubusercontent.com/Logible/Image/main/note_image/20230927210007.png)
