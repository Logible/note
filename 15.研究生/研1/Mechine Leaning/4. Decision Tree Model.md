# 4. Decision Tree Model

- one example![Alt text](images/image-28.png)
- pick one![Alt text](images/image-27.png)

## Learning Process

1. ![Alt text](images/image-29.png)
2. ![Alt text](images/image-30.png)

## Measuring purity

- Entropy as a measure of impurity
  - ![Alt text](images/image-31.png)
  - ![Alt text](images/image-32.png)

## Choosing a split: information Gain

- information Gain
  - ![Alt text](images/image-33.png)
  - ![Alt text](images/image-34.png)

## Decision Tree Learning

- ![Alt text](images/image-35.png)
- ![Alt text](images/image-36.png)

## Using one-hot encoding ofcategorical features

- ![Alt text](images/image-37.png)
- ![Alt text](images/image-38.png)

## Splitting on a continuous variable

![Alt text](images/image-39.png)

## Regression Trees (optional)

- ![Alt text](images/image-40.png)

## Using multiple decision trees

- ![Alt text](images/image-41.png)
- ![Alt text](images/image-42.png)

## Sampling with replacement

![Alt text](images/image-43.png)

## Random forest algorithm

![Alt text](images/image-44.png)

## XGBoost

- XGBoost (extreme Gradient Boosting)
  - ![Alt text](images/image-45.png)

## When to use decision trees

- ![Alt text](images/image-46.png)
