# 1week

## 0920

1. optimize wind turbine power generation
2. come off the **assembly line**
3. state of the art
4. When I founded and was leading the Google Brain Team
5. a langding AI
6. hope you **stick with me** through this class
7. tumor
   1. benign
   2. malignant
8. **crosses** to show the patients with a tumor that was malignant
9. So to **recap** supervised learning maps input x to output y

- wind turbine![wind turbine](https://upload.wikimedia.org/wikipedia/commons/b/ba/Windmills_D1-D4_%28Thornton_Bank%29.jpg)
- assembly line![assembly line](https://www.investopedia.com/thmb/ti3Mugu8sGx4iqJL0UzFhsA5Vx8=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc()/AssemblyLine3-2-c9c005f8e3db48e4975a3172098852b2.jpg)

## 0925

1. When executing a "Code" cell, its code is sent to a execution **kernel**.
2. **hammock**
3. **concretely**, if you take that point
4. a slightly **hilly** outdoor park
5. a **golf course** where the high points are
6. this equal sign is the **assignment operator**
7. about how to correctly in **semantic** gradient descent,
8. you want to **simultaneously** update w and b

- ![hammock](https://i.etsystatic.com/18643702/r/il/00126c/4398914046/il_fullxfull.4398914046_gv56.jpg)
- a steep gradient![a steep gradient](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRL-6WPv15ozFudu3CKjY0d7BwEsVcrfp5p8OTpBXWl&s)

## 0926

1. This means you **subtract** from w, a negative number.
2. So this is a tricky one.
3. you'll be **two thirds** of the way
4. and sometimes to **designate** that this is a vector
5. GPU hardware that **stands for** graphics processing unit
6. this still doesn't use **factorization**.
7. is much faster is **behind the scenes**.
8. the derivative of the cost J **with respect to** $w_1$.
9. with this being the Greek **alphabets** Mu.
10. As a rule of thumb
11. you can try to **spot** whether or not
12. **To fix this**,you can use a smaller learning rate
13. It turns out that is one **flavor** of feature engineering
14. and the third feature is the size **cubed**.
    1. These two features, x squared and x **cubed**
15. it was strangely **exhilarating**.
16. We often designate **clauses** as no or yes
17. and this will turn out to be the same z as the one you saw on the previous **slide**.
18. the loss function **incentivizes/ɪnˈsentɪvaɪz/** or **nurtures/ˈnɜːtʃə/**
    1. plants **nurtured** in the greenhouse
19. But there's a bowl of **porridge/ˈpɒrɪdʒ/** that is neither too cold nor too hot.

- subtract![s](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8b/Subtraction01.svg/1200px-Subtraction01.svg.png)
- tricky![tricky](https://thumbs.dreamstime.com/z/%E6%A3%98%E6%89%8B%E9%97%AE%E9%A2%98-29439783.jpg)

## 0927

1. doing something as **harsh** as eliminating it **outright**.
2. function that's less **prone** to overfitting
3. And so this new cost function **trades off** two goals that you might have.
4. and then it **fell out of favor** for a while.
5. **tremendous** traction in some applications
6. trying to build software to **mimic** the brain
7. it has a number of inputs where it receives electrical impulses from other **neurons**.
8. I'd like to give one big **caveat**,
9. which is **frankly** very little
10. that neural networks have really taken off?

- neurous![20230927211450](https://raw.githubusercontent.com/Logible/Image/main/note_image/20230927211450.png)
- nucleus![20230927211550](https://raw.githubusercontent.com/Logible/Image/main/note_image/20230927211550.png)
