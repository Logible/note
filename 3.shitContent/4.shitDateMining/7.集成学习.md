# 集成学习

1）集成学习的一般框架及要素|p435-436,y2
2）Bagging算法（算法流程）|p439-446,y8|P173-175
3）AdaBoost算法（基本思想）|p447-456,y10|P175-178

## 集成学习的要素

要素:

![20220102193531](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220102193531.png)

一般框架:

![20220102193646](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220102193646.png)

## Bagging算法

Bagging 又称为 boot strap aggregating, 是一种根据均匀概率分布从数据集中重复抽样(有放回)的技术

![20220102201400](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220102201400.png)

## AdaBoost算法

AdaBoost将每一个分类器$C_j$的预测值根据$a_j$进行加权，而不采用多数表决的方案，这种机制有助于AdaBoost惩罚那些准确率很差的模型，如那些在较早提升轮产生的模型。另外，如果任何中间伦产生高于50%的误差，则权值将被恢复为开始的一致值$w_i = \frac{1}{n}$

并在每轮训练中增加被错误分类样本的权值，并减少已经被正确分类的样本的权值

最终预测结果通过取每个基分类器的加权平均得到

---
question: why 0.9 not boost???

why the back not be effected???

---
