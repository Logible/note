# 绪论

1. 数据挖掘任务
2. 数据挖掘主要流程

## 1.数据挖掘任务

- 预测任务

   预测任务主要包括回归和分类

- 描述任务

   描述任务主要包括关联分析、聚类分析、异常检测等

- 四种主要的挖掘任务

1. 聚类分析(cluster analysis)
   1. Given a set of data points, each having a set of attributes, and a similarity measure among them, find clusters such that
2. 预测建模(predictive modeling)
   1. 分类(classification): 用于预测离散的目标变量
   2. 回归(regression): 用于预测连续的目标变量
3. 关联分析(association analysis):
   1. 用来发现描述数据中强关联特征的模式
4. 异常检测(anomaly detection)
   1. 识别其特征不同于其他数据的观测值

## 2.数据挖掘的主要流程

1. 分析问题，定义挖掘目标
2. 数据采集
3. 数据预处理
4. 构建挖掘模型
5. 模型评价优化
6. 模型部署应用

# 第二章 数据

1. 不同的属性类型
2. 数据质量问题
3. 数据预处理的主要方法
4. 连续属性离散化
5. 相似性/相关性度量：**距离、余弦、SMC、Jaccard系数**、皮尔森相关系数

## 1. 不同的属性类型

1. 属性

    属性(attribute): 对象的性质或特性

    属性也称:变量、特性、字段 or 维

2. 测量标度

    测量标度(measurement scale): 将数值和符号值与对象的属性相关联的规则(函数)

3. 属性的类型(测量标度的类型)

    取决于下列4种性质

    - Distinctness(相异性): = ≠
    - Order(序):<>
    - Addition: + -
    - Multiplication(乘法): * /

    结合四种属性,可定义四种属性类型

    1. 分类的(定性的)

        - 标称
        - 序数

    2. 数值的(定量的)
        - 区间
        - 比率

        ![20211223205517](https://raw.githubusercontent.com/Logible/Image/main/note_image/20211223205517.png)

## 2. 数据质量问题

### 2.1.2 数据集的类型

1. 数据集的一般特性

   - 维度(Dimensionality)
     - 维度是数据集中的对象具有的属性数目
     - 维灾难(curse of dimensionality)
     - 维归约(dimensionality reduction)
   - 稀疏性(sparsity)
     - 一个对象大部分属性上的值为0
     - 只存储和处理非零值
   - 分辨率(resolution)
     - 数据的模式依赖于分辨率——度量尺度 (scale)

2. 数据集类型

   - 记录数据(record)
     - 数据矩阵(Data Matrix)
     - 文本数据(Document Data)：每篇文档可以表示成一个文档-词矩阵
     - 事务数据(Transaction Dat)
   - 基于图形的数据(graph)
     - World Wide Web
     - 分子结构（Molecular Structures）
   - 有序数据(ordered)
     - 空间数据(Spatial Data)
     - 时间数据(Temporal Data)
     - 序列数据(Sequential Data )

## 2.2 数据质量

### 2.2.1 测量和数据收集问题

- 测量误差和数据收集错误
- 噪声和伪像
- 精度、偏倚、准确率
- 离群点
- 遗漏值
- 不一致的值
- 重复的值

## 2.3 数据预处理

数据预处理的主要方法

- 聚集 (Aggregation):Combining two or more attributes (or objects) into a single attribute (or object)
- 抽样 (Sampling):是一种选择数据对象子集进行分析的常用方法
- 维归约 (Dimensionality Reduction)
- 特征子集选择 (Feature subset selection)
  - emmbedded approach
  - wrapper approach
  - filter approach
- 特征创建 (Feature creation)
  - 特征提取(Feature Extraction)
  - 映射数据到新的空间(Mapping Data to New Space)
  - 特征构造(Feature Construction)
- 离散化与二元化 (Discretization and Binarization)
  - 离散属性二元化
  - 连续属性离散化
- 属性变换 (Attribute Transformation)
  - 简单变换
  - 标准化(standardization)或规范化(normalization)

连续属性二元化:

explain:

类信息:

In set theory and its applications throughout mathematics, a class is a collection of sets (or sometimes other mathematical objects) that can be unambiguously defined by a property that all its members share.

使用类信息(supervised)还是不使用类信息(unsupervised)

- 非监督离散化
- 监督离散化

![20211224105403](https://raw.githubusercontent.com/Logible/Image/main/note_image/20211224105403.png)

## 2.4 相似性和相异度的度量

### 2.4.3 数据对象之间的相异度

距离: 具有特定性质的相异度

Euclidean distance(欧式距离)

d(x,y) = $\sqrt  {\sum_{k=1}^{n} {(x_k-y_k)^2}}$

欧几里得距离可以用Minkowski distance来推广

$d(x,y) = \left(\sum_{k=1}^{n} {|x_k-y_k|^r}\right)^{\frac{1}{r}}$

- r = 1, Manhantann distance
- r = 2, Euclidean distance
- r = ∞, Supremun distance

$d(x,y) =\lim_{x \to \infty} \left(\sum_{k=1}^{n} {|x_k-y_k|^r}\right)^{\frac{1}{r}} = max\{|x_k-y_k|,k=1,2, \dots, n\}$

距离的性质

1. 非负性
2. 对称性
3. 三角不等式

满足以上三个性质称为度量(metric)

有些相异度无法满足度量性质

### 2.4.5 邻近性度量的例子

x,y为两个对象, 都由n个二元属性组成

- $f_{00}$: x取0 y取0
- $f_{01}$: x取0 y取1
- $f_{10}$: x取1 y取0
- $f_{11}$: x取1 y取1

1. 简单匹配系数(Simple Matching Coefficient)

    该度量对出现和不出现都进行计数

    SMC = $\frac{f_{11}+f_{00}}{f_{01}+f_{10}+f_{11}+f_{00}}$

    SMC可用于是非题就按测回答问题相似学生

2. Jaccard系数(Jaccard Coefficient)

    Jaccard 假定x和y是两个数据对象，代表一个事物矩阵的两行,忽略0-0匹配

    J = $\frac{f_{11}}{f_{01}+f_{10}+f_{11}}$

3. 余弦相似度
  
    忽略0-0匹配 && 处理非二元向量

    $\cos(x,y) = \frac{x · y}{||x|| ||y||} = x'y'$

     - $||x||$为向量x的长度
     - $x'=\frac{x}{||x||}$: 长度为1的向量

    ![20220103213543](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220103213543.png)

4. 皮尔森相关系数

    explaination:

    线性: Correlation is said to be linear if the ratio of change is constant

    ![20211226163041](https://raw.githubusercontent.com/Logible/Image/main/note_image/20211226163041.png)

    对象之间的相关性是对象属性之间**线性**联系的度量

    $corr(x,y) = \frac{covariance(x,y)}{standard_deviation(x)× standard_deviation(y)} = \frac{s_{xy}}{s_xs_y}$

    ![20211226164257](https://raw.githubusercontent.com/Logible/Image/main/note_image/20211226164257.png)

    ---
    to be studied
    stardard_deviation and covariance

    ---

![20220103213815](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220103213815.png)

# 第三章 探索数据

1. 汇总统计：众数、百分位数、中位数、极差
2. 可视化：盒状图

## 3.2 汇总统计

### 3.2.1 频率和众数

$frequency(v_i) = \frac{The\ object\ number\ has\ property\ vi}{m}$

众数：最高频率的值

### 3.2.2 百分位数 percentile

percentile:] is a score below which a given percentage k of scores in its frequency distribution falls (exclusive definition) or a score at or below which a given percentage falls (inclusive definition).

### 3.2.3 均值和中位数

$mean(x) = \overline{x} = \frac{1}{m}\sum_{i-1}^{m}x_i$

$median(x) = x_{r+1}: m = 2r+1$

$median(x) = \frac{1}{2}(x_r+x_{r+1}): m = 2r$

### 3.2.4 散布度量：极差和方差

连续数据的另一组常用的汇总统计是值集的**divergence(弥散)度量**

此度量表面属性是否散布很宽，或者是否相对集中在单个点(如均值)附近

极差(range): $range_{(x)} = max_{(x)} - min_{(x)} = x_{(m)} - x_{(1)}$

![20211226203209](https://raw.githubusercontent.com/Logible/Image/main/note_image/20211226203209.png)

![20211226203203](https://raw.githubusercontent.com/Logible/Image/main/note_image/20211226203203.png)

## 3.3 可视化

### 3.3.3 技术

盒状图:

盒的下端和上端分别指示第25和第75个百分位数，而盒中的线指示第50个百分位数的值，底部和顶部的尾线分别指示第10和第90个百分位数，离群值用“+”显示

![20211226204057](https://raw.githubusercontent.com/Logible/Image/main/note_image/20211226204057.png)

# 第四章 分类

1. 建立分类模型的一般方法
2. 决策树算法
    - 1）算法框架
    - 2）划分评估：基于熵的方法、基于Gini系数的方法、基于分类误差的方法

3. 分类模型的评估方法 （部分内容见课本的第5章

- 1）混淆矩阵|p303-304,y2|P91-92
- 2）各种性能指标
  - p303-311,y9
  - 准确率|P91
  - 错误率|P91
  - Precision|P181-182
  - Recall|P181-182
  - P-R曲线|p309
  - ROC曲线等高|P182-184

- 3）评估方法，包括保留法，K-折交叉验证法、自助法等。|p312-317,y6|P114-115

分类任务:

通过学习得到一个目标函数(target function), 把每个属性集x映射到一个预先定义的类标号y

## 4.2 解决分类问题的一般方法

建立分类模型的一般方法

- 基本方法
  - Logistic Regression（逻辑回归）
  - Decision Tree based Methods（决策树）
  - Rule-based Methods（基于规则的方法）
  - Nearest-neighbor（近邻方法）
  - Neural Networks（神经网络）
  - Deep Learning（深度学习）
  - Naïve Bayes and Bayesian Belief Networks（贝叶斯方法）
  - Support Vector Machines（支持向量机）
- 集成方法
  - Boosting（提升）
  - Bagging（装袋）
  - Random Forests（随机森林

## 4.2Next 解决分类问题的一般方法

1. 混淆矩阵(confusion matrix)

    定义:

    a confusion matrix, also known as an error matrix,  is a specific table layout that allows visualization of the performance of an algorithm,

    ![20211229124129](https://raw.githubusercontent.com/Logible/Image/main/note_image/20211229124129.png)

    - TPR = $\frac{TP}{TP+FN}$
    - TNR = $\frac{TN}{FP+TN}$

    - FPR = $\frac{FP}{FP+TN}$
    - FNR = $\frac{FN}{TP+FN}$

    accuracy = $\frac{f_{11}+f_{00}}{f_{11}+f_{10}+f_{01}+f_{00}} = \frac{TP+TN}{N}$

    error_rate = $\frac{FP+FN}{N}$

    Precision = $\frac{TP}{TP+FP}$

    Recall(TPR) = $\frac{TP}{TP+FN}$

2. ROC曲线

    接受者操作特征(receive operating characteristic, ROC)曲线是显示分类器真正率(TPR)和假正率(FPR)之间折中的一种图形化方法

    关键点

    - (TPR = 0.FPR =0):把每个实例都预测为负类的模型。
    - (TPR = 1, FPR= 1):把每个实例都预测为正类的模型。
    - (TPR = 1, FPR = 0):理想模型。

    等高: 有相同的TPR,但FPR逐渐变大

    ---
    question:

    ![20211229161934](https://raw.githubusercontent.com/Logible/Image/main/note_image/20211229161934.png)

    ---

    ![20220101114246](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220101114246.png)

3. P-R曲线

    Typically, Precision and Recall are inversely related, ie. as Precision increases, recall falls and vice-versa. A balance between these two needs to be achieved by the IR system, and to achieve this and to compare performance, the precision-recall curves come in handy.

    ![20211229162455](https://raw.githubusercontent.com/Logible/Image/main/note_image/20211229162455.png)

    书本未找到

## 4.3 决策树归纳

### 4.3.1 决策树的工作原理

决策数是一种由结点和有向边组成的层次结构,包括

- root node
- internal node
- leaf node or terminal node

### 4.3.3 表示属性测试条件的方法

- 二元属性
- 标称属性
- 序数属性
- 连续属性

### 4.3.4 选择最佳划分的度量

选择最佳划分的度量通常是根据划分后子女结点不纯(impurity)性的程度

impurity的程度越低, 类分布就越倾斜

选择增益最大的部分

增益: $\Delta = I(parent) - \sum_{j=1}^{k}\frac{N(v_j)}{N}I(v_j)$

- $N(v_j)$是与子女结点$v_j$相关联的记录个数

Gini:

![20211228170644](https://raw.githubusercontent.com/Logible/Image/main/note_image/20211228170644.png)

Entropy:

![20211228170704](https://raw.githubusercontent.com/Logible/Image/main/note_image/20211228170704.png)

计算熵时, $Olog_20 = 0$

Classification error:

![20211228170741](https://raw.githubusercontent.com/Logible/Image/main/note_image/20211228170741.png)

增益率Gain ratio:

![20211228172728](https://raw.githubusercontent.com/Logible/Image/main/note_image/20211228172728.png)

SplitINFO

![20211228172821](https://raw.githubusercontent.com/Logible/Image/main/note_image/20211228172821.png)

---
question:

决策树算法框架

![20211228173147](https://raw.githubusercontent.com/Logible/Image/main/note_image/20211228173147.png)

---

## 4.5 评估分类器的性能

hold out与k-fold cross validation方法的不足:

由于保留一部分样本用于检验，实际的训练集比D小，因此会引入一些因训练样本规模不同而导致的估计偏差。
leave-one-out方法受训练规模变化的影响较小，但是计算复杂度又太高 。

1. 保持法(hold out)

    将数据集D划分为两个互斥的集合，其中一个作为训练集S，另一个作为检验集T

2. 随机二次抽样

    多次重复保持方法来改进对分类器性能的估计

3. 交叉验证(cross-validation)

   - 二折交叉验证

   选择一个子集作训练集, 而另一个作检验集, 然后交换两个集合的角色

   - k折交叉验证

   把数据分为大小相同的k份, 在每次运行, 选择其中一份作检验集, 该过程重复k次, 使得每份数据都用于检验恰好一次.

   - Leave-one-out（留一法）

   k=n时的k-fold cross validation

4. 自助法(bootstrapping)

    训练记录采用有放回抽样, 即已经选作训练的记录将放回原来的记录集中, 使得它等几率地被重新抽取.

    对于n个样本组成的数据集D，有放回抽样n次，则一个样本始终未被抽中的概率是$(1-\frac{1}{n})^n$，当n足够大时，$(1-\frac{1}{n})^n \to \frac{1}{e} ≈ 0.368$

# 第五章 贝叶斯分类器

1）贝叶斯公式|p350,y2|P139-140

2）贝叶斯分类的一般方法-?

3）朴素贝叶斯分类器|p319-329,330-342,y24|P141-145

## 5.3 贝叶斯分类器

![20220104100935](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220104100935.png)

Bayes theorem: 一种把类的先验知识和从数据中收集的新证据相结合的统计原理

条件概率:一随机变量在另一随机变量取值已知的情况下取某一特定值的概率

![20220101170627](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220101170627.png)

P(Y=y|X=x): 变量X取值x的情况下, 变量Y取值y的概率

X和Y的联合概率和条件概率满足:

$\displaystyle P(X,Y) = P(Y|X) × P(X) = P(X|Y) × P(Y)$

ie.

$P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}$

## 5.3.2 贝叶斯定理在分类中的应用

X表示属性集, Y表示类表量

P(Y|X): Y的后验概率 posterior probability

P(Y): Y的先验概率 prior probability

$P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}$

P(X)为常数

P(Y)可以通过训练记录占比估计

分类器任务:

给定一个记录对应的属性向量X，判断其对应的类标签y

![20220101213106](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220101213106.png)

### 5.3.3 朴素贝叶斯分类器

在计算条件概率时, 假设属性之间是条件独立的,进而,将联合概率的计算简化成边缘概率的计算(朴素)

1. 条件独立性

    ![20220101221623](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220101221623.png)

    类条件概率$\prod_{i}P(X_i|Y)$

2. 离散属性计算

    - 首先，将数据集中属于第j个类别的记录筛选出来，相应的记录个数为Nj；
    - 其次，统计其中第i个属性的取值为xi的记录个数mi;

    $P(xi | y=cj) = \frac{mi}{Nj}$

3. 条件概率的m估计(m-estimate)

    如果有一个属性的类条件概率等于0, 则整个类的后验概率就等于0

    解决该问题的途径是使用m估计

    $\displaystyle P(x_i) = \frac{n_c+mp}{n+m}$

# 第五章 人工神经网络ANN artificial neural networks

1）人工神经网络的要素：网络结构、激活函数、优化模型（损失函数）、优化算法

2）感知器的网络结构以及学习算法

3）三层前馈神经网络：网络结构、激活函数、基于均方误差损失函数的优化模型

决定人工神经网络性能的三大要素

1. 神经元的特性(**激活函数**)
2. 神经元之间相互连接的形式(**拓扑结构**)
3. 为适应环境而改善性能的**学习规则**

## 5.4.1 感知器

- 神经元：sign()
- 网络结构：两层
- 训练规则：给定训练数据集D，如何确定最优的w和b

感知器:

- 几个输入结点, 用来表示输入属性
- 一个输出结点, 用来提供模型输出
  - 输出结点是一个数学装置, 计算输入的加权和, 减去偏置项, 然后根据结果的符号产生输出
  - 数学方式表示:

---
ques:<=0?

因为<=0说明判断的类和预测的类相反

---

![20220102013945](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220102013945.png)

![20220102011314](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220102011314.png)

![20220104100012](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220104100012.png)

## 5.4.2 多层人工神经网络

前馈神经网络: 每一层的结点仅和下一层的结点相连

1. 网络结构

   - 输入层Input Layer
   - 隐藏层Hidden Layer
     - 中间的结点称为隐藏结点(hidden node)
   - 输出层Output Layer

2. 激活函数

    神经元中使用的激活函数(activation function), 除了符号函数之外, 还可以使用其它激活函数, those activation function 允许隐藏结点和输出结点的输出值与输入参数呈非线性关系

    ![20220102160407](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220102160407.png)

3. 损失函数

    当结构给定的情况，神经网络对应的函数由参数（连接权重）W决定，即参数化的函数fW()

    神经网络的训练,本质上是要找到合适参数W

    1. 首先，需要有一个度量来衡量参数W的好坏，这个度量通常体现为损失函数loss(W)。
    2. 其次，在损失函数的基础上，将参数W的确定归结为一个优化问题。

    ![20220102162950](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220102162950.png)

    ![20220102162937](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220102162937.png)

    均方误差损失函数:

    ![20220102163154](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220102163154.png)

    ![20220102164659](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220102164659.png)

    ![20220102164641](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220102164641.png)

    ![20220102164649](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220102164649.png)

# 第五章 集成学习

1）集成学习的一般框架及要素|p435-436,y2

2）Bagging算法（算法流程）|p439-446,y8|P173-175

3）AdaBoost算法（基本思想）|p447-456,y10|P175-178

## 集成学习的要素

要素:

![20220102193531](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220102193531.png)

一般框架:

![20220102193646](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220102193646.png)

## Bagging算法

Bagging 又称为 boot strap aggregating, 是一种根据均匀概率分布从数据集中重复抽样(有放回)的技术

![20220102201400](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220102201400.png)

## AdaBoost算法

AdaBoost将每一个分类器$C_j$的预测值根据$a_j$进行加权，而不采用多数表决的方案，这种机制有助于AdaBoost惩罚那些准确率很差的模型，如那些在较早提升轮产生的模型。另外，如果任何中间伦产生高于50%的误差，则权值将被恢复为开始的一致值$w_i = \frac{1}{n}$

并在每轮训练中增加被错误分类样本的权值，并减少已经被正确分类的样本的权值

最终预测结果通过取每个基分类器的加权平均得到

---
question: why 0.9 not boost???

why the back not be effected???

---

# 第六七章 关联分析

1. 基本概念：

    项集、K-项集、支持度计数、支持度、频繁项集、关联规则、关联规则的支持度和置信度

2. 先验原理

3. 产生频繁项集的Apriori算法

    - 1）算法框架
    - 2）产生候选频繁项集的Fk-1Fk-1方法

4. 序列模式发现

    - 1）序列的基本概念
    - 2）子序列与序列包含
    - 3）序列模式发现的类Apriori算法

---

关联分析(association analysis):

用于发现隐藏在大型数据集中的有意义的联系,所发现的联系可以用关联规则(association rule)或频繁项集的形式表示

例如: {尿布}->{啤酒}

## 6.1 问题定义

- 项集(Itemset)

    a set include zero or multiple items

- k-项集

    including k items

- 项集支持度计数(Support count)

    $σ(X) = |{t_i|X\subseteq t_i,t_i\in T }|$

- 项集支持度(Support)

    $s = σ(X)/amount$

关联规则:形如 X->Y 的蕴含表达式

1. 支持度(support):给定数据集的频繁程度

    $\displaystyle s(X->Y) = \frac{σ(X\cup Y)}{N}$

2. 置信度(confidence):确定Y在包含X的事务中出现的频繁程度

    $\displaystyle c(X->Y) = \frac{σ(X\cup Y)}{σ(X)}$

关联规则挖掘问题:

给定事务的集合T,找出支持度≥ minsup并且置信度≥minconf的所有规则

大多数关联规则挖掘算法通常采用的策略是, 将关联规则挖掘任务分解为如下两个主要的子任务

1. 频繁项集产生(Frequent Itemset Generation)

    发现满足最小**支持度**阈值的所有项集, 这些项集称作频繁项集(frequent itemset)

2. 规则的产生(Rule Generation)

    从上一步发现的频繁项集中提取所有高**置信度**的规则, 这些规则称作强规则(strong rule)

### 6.2 频繁项集的产生

一个包含k个项的数据集可能产生$2^k-1$个频繁项集, 不包括空集

有几种方法可以降低产生频繁项集的计算复杂度

1. 减少候选项集的数目(M):如先验原理
2. 减少比较次数

### 6.2.1 先验原理

定义: If a simple pattern is not supported, then a more complicated one with that simple pattern in it can not be supported (e.g. if AC isn't supported, there is no way that ABC is supported)

This process of removing patterns that can't be supported because their subsets (or shorter combination) aren't supported is called pruning. (support-based pruning)

![20211227152635](https://raw.githubusercontent.com/Logible/Image/main/note_image/20211227152635.png)

### 6.2.2 Apriori算法的频繁项集产生

```py
k = 1
{发现所有频繁1-项集}
repeat
    k = k + 1
    {产生候选项集}
    for 每个事务 t ∈ T do
        {识别属于t的所有候选}
        for 每个候选项集 c ∈ Ct do
           {支持度技术增值}
        end for
    end for
   {提取频繁k-项集}
until Fk = ∅
Result = ∪ Fk
```

算法描述:

- $F_k$: frequent k-itemsets (频繁k-项集的集合)
- $C_k$: candidate k-itemsets (候选k-项集的集合)

---
Let k = 1

Generate F1 = {frequent 1-itemsets}

Repeat until $F_k$ is empty

- k = k + 1

- Candidate Generation: Generate $C_k$ from $F_k-1$
- Candidate Pruning: Prune candicate itemsets in $C_k$ containing subsets of length k-1 that are infrequent
- Support Counting: Count the support of each candidate in $C_k+1$ by scanning the DB
- Candicate Elimination: Eliminate candidates in $C_k$ that are infrequent, leaving only those that are frequent=>Fk

Output frequent itemsets $\cup F_k$

---

候选集产生方法-要求

1. 避免产生太多不必要的候选
2. 确保候选项集的集合是完全的
3. 不产生重复候选项集

$F_{k-1} × F_{k-1}$

Merge two frequent (k-1)-itemsets if the last (k-2) items of the first one is identical to the first (k-2) items of the second.

在两个频繁的Fk-1中,一个截去头部一个截去尾部,再将头尾补回Fk-2中,即形成Fk

question:

![20211227190200](https://raw.githubusercontent.com/Logible/Image/main/note_image/20211227190200.png)

UNDERSTAND

![20211227201627](https://raw.githubusercontent.com/Logible/Image/main/note_image/20211227201627.png)

### 7.4 序列模式

### 7.4.1 问题描述

序列: 将与对象A有关的所有时间按时间戳增序排序, 就得到对象A的一个序列(sequence)

子序列: 如果t中每个有序元素都是s中一个有序的子集,序列t是另一个序列s的子序列(subsequence).

ie..:存在整数$1 \le j_1 \le j_2 \dots \le j_m \le n$, 使得$t_1 \subseteq s_{j_1}, t_2 \subseteq s_{j_2},\dots,t_m \subseteq s_{j_m}$

### 7.4.2 序列模式发现

数据序列: 是指与单个数据对象相关联的事件的有序列表

序列s的支持度: 包含s的所有数据序列所占的比例

序列模式发现:给定序列数据库D和用户指定的最小支持度阈值minsup，序列模式发现的任务是找出支持度大于或等于minsup的所有序列 。

序列模式发现的类Apriori算法

![20211227222207](https://raw.githubusercontent.com/Logible/Image/main/note_image/20211227222207.png)

# 第八章 聚类分析

1. 聚类分析的任务描述
2. 聚类任务（无监督学习）与分类任务（有监督学习）的不同
3. K-均值聚类算法
4. 凝聚层次聚类算法
5. DBSCAN算法的基本原理

## 聚类分析

将数据对象分组，使得同一组内的对象彼此相似（或相关），而不同组中的对象是不同的（或不相关）；组内的相似性（同质性）越大，组间差别越大，则聚类（分组）越好。

## 聚类任务（无监督学习）与分类任务（有监督学习）的不同

- 聚类是非监督分类(unsupervised classification), 它用类(簇)标号创建对象的标记. 然而, 只能从数据导出这些标号.

- 相比之下, 分类任务是监督分类(supervised classification), 即使用由类标号已知的对象开发的模型.

### 8.1.3 不同的簇类型

- K均值: 基于原型的、划分的聚类技术。试图发现用户指定个数(K)的簇(由质心代表 簇的原型通常是质心，即簇中所有点的平均值)
- 凝聚的层次聚类：开始，每个点作为一个单点簇；然后，重复地合并两个最靠近的簇，直到产生单个的、包含所有点的簇。其中某些技术可以用于基于图的聚类接受，而另一些可以用基于原型的方法解释
- DBSCAN：一种产生划分聚类的基于密度的聚类算法，簇的个数由算法自动地确定。低密度区域中的点被视为噪声而忽略，因此DBSCAN不产生完全聚类

### K-均值聚类算法

首先，选择K个初始质心，其中K为用户指定的参数，即所期望的簇的个数。每个点指派到最近的质心，而指派到一个质心的点集为一个簇。然后根据指派到簇的点，更新每个簇的质心，直到簇不发生变化。

![20220103165929](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220103165929.png)

### 凝聚的层次聚类

![20220103175001](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220103175001.png)

- 单链：两个簇的邻近度定义为两个不同簇中任意两点之间最短距离（最大相似度）
- 全链：两个簇的邻近度定义为两个不同簇中任意两点之间最长距离（最小相似度）

### DBSCAN是基于密度的算法

- 密度: 指定半径Eps内的点数

- 核心点: 如果点X的密度超过给定的阈值MinPts(这些是在聚类内部的点

- 边界点: 点X的密度不超过给定的阈值MinPts，但是落在某个核心点的Eps邻域内(这些点在核心点附近

- 噪声点指核心点或边界点之外的其它点。

算法：

- 1）将所有点标记为核心点、边界点或噪声点
- 2）删除所有的噪声点
- 3）对剩余点执行聚类

# 第十章 异常检测

1.异常检测的应用
2.离群点
3.异常检测的主要技术方法

## 1. 异常检测的应用

1. 欺诈检测
2. 天气预测
3. 电子商务
4. 公共安全
5. 入侵检测
6. 医疗

## 2.离群点

异常对象通常也叫“离群点”（outlier）

- Hawkins：异常是在数据集中偏离大部分数据的数据，使人怀疑这些数据的偏离并非由随机因素产生，而是产生于完全不同的机制。
- Weisberg：异常是与数据集中其余部分不服从相同统计模型的数据。
- Samuels：异常是足够地不同于数据集中其余部分的数据。
- Porkess：异常是远离数据集中其余部分的数据

## 3.异常检测的主要技术方法

1. 按类标号（正常/异常）利用的程度

   - 无监督的异常检测方法：没有提供类标号
   - 有监督的异常检测方法：存在异常点类和正常类的训练集
   - 半监督的异常检测方法：训练数据包含被标记的正常数据，但是没有关于异常对象的信息

2. 按使用的主要技术路线角度

    - 基于统计的方法
    - 基于邻近度的方法
    - 基于密度的方法
    - 基于聚类的方法

![20220103204325](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220103204325.png)

![20220104104234](https://raw.githubusercontent.com/Logible/Image/main/note_image/20220104104234.png)

# 附录 回归

1. 线性回归
2. Logistic回归

回归定义:回归的目标是找到一个可以最小误差拟合输入数据的目标函数.

回归任务的误差函数(error function)可以用绝对误差/平方误差和表示

- 绝对误差: $\sum{i}|y_i - f(x_i)|$
- 平方误差: $\sum{i} \left( y_i - f(x_i) \right)^2$

## D.2 简单线性回归

假设目标函数是线性函数: $y = f(x) = β_0 + β_1x$

![20211228125525](https://raw.githubusercontent.com/Logible/Image/main/note_image/20211228125525.png)

![20211228125535](https://raw.githubusercontent.com/Logible/Image/main/note_image/20211228125535.png)

### Logistic回归

In statistics, the logistic model (or logit model) is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick. This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc.

![20211228135859](https://raw.githubusercontent.com/Logible/Image/main/note_image/20211228135859.png)

![20211228135921](https://raw.githubusercontent.com/Logible/Image/main/note_image/20211228135921.png)
